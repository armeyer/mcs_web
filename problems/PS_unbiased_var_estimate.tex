\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{PS_unbiased_var_estimate}
  \pcomment{Velleman}
  \pcomment{F01.ps12, S98.ps11, F97.ps11}
  \pcomment{\textbf{An Unbiased Estimator}}
\end{pcomments}

\pkeywords{
  random_variable
  expectation
  unbiased
  variance
  estimate
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
Suppose we are trying to estimate some physical parameter $p$.  When
we run our experiments and process the results, we obtain an estimator
of $p$, call it $p_e$.  But if our experiments are probabilistic, then
$p_e$ itself is a random variable\iffalse which has a PDF over some
range of values\fi.  We call the random variable $p_e$
an \emph{unbiased} estimator if $\expect{p_e}=p$.

For example, say we are trying to estimate the height, $h$, of Green
Hall.  However, each of our measurements has some noise that is, say,
Gaussian with zero mean.  So each measurement can be viewed as a
sample from a random variable $X$.  The expected value of each
measurement is thus $\expect{X} = h$, since the probabilistic noise
has zero mean.  Then, given $n$ independent trials, $x_1, \dots, x_n$,
an unbiased estimator for the height of Green Hall would be
\[
h_e = \frac{x_1+\cdots+x_n}{n},
\]
since
\[
\expect{h_e} = \expect{\frac{x_1+\cdots+x_n}{n}}
= \frac{\expect{x_1} + \cdots + \expect{x_n}}{n} = \expect{x_1}= h.
\]

Now say we take $n$ independent observations of a random variable $Y$.
Let the true (but unknown) variance of $Y$ be $\variance{Y}
= \sigma^2$.  Then \iffalse
(see section 6.4 in the
\href{http://theory.lcs.mit.edu/classes/6.042/fall01/lectures/l13.pdf}{notes})\fi,
we can define the following estimator $\sigma_e^2$ for $\variance{Y}$
using the data from our observations:
\[
\sigma_e^2 =  \frac{y_1^2 + y_2^2 + \cdots + y_n^2}{n} -
                \left(\frac{y_1 + y_2 + \cdots + y_n}{n}\right)^2.
\]

%\begin{eqnarray*}
%\variance{Y}
%%        & = & \expect{Y^2} - \expectsq{Y} \\
%%        & \approx & \frac{y_1^2 + y_2^2 + \cdots + y_n^2}{n} -
%                \left(\frac{y_1 + y_2 + \cdots + y_n}{n}\right)^2.
%\end{eqnarray*}

Is this an unbiased estimator of the variance?   In
other words, is $\expect{\sigma_e^2} = \sigma^2$?   If not, can you
suggest how to modify this estimator to make it unbiased?

\begin{solution}
Let $\sigma^2=\variance{X}, \mu=\expect{X}$.  Then our estimator
$\sigma^2_e$ is given by
\begin{eqnarray*}
\sigma^2_e&=&\frac{\sum
  y_i^2}{n} - \left(\frac{\sum y_i}{n}\right)^2\\
\expect{\sigma^2_e}&=&\expect{\frac{\sum
  y_i^2}{n} - \left(\frac{\sum y_i}{n}\right)^2}\\
&=&\frac{\sum
  \expect{y_i^2}}{n} - \frac{\expect{(\sum y_i)^2}}{n^2}\\
&=&\frac{\sum(\sigma^2+\mu^2)}{n} - \frac{\variance{\sum
    y_i}+\expectsq{\sum y_i}}{n^2}\\ 
&=&\frac{n(\sigma^2+\mu^2)}{n} - \frac{n\sigma^2+n^2\mu^2}{n^2}\\
&=&\sigma^2\left(1-\frac{1}{n}\right)
\end{eqnarray*}
So this gives a biased estimator, but we can make it
unbiased simply by multiplying by $\frac{n}{n-1}$.
\begin{eqnarray*}
\expect{\frac{n\sigma^2_e}{n-1}}&=&\sigma^2
\end{eqnarray*}

\end{solution}

\end{problem}

\endinput
