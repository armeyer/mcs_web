\documentclass[problem]{mcs}

\begin{pcomments}
  \pcomment{PS_variance_additivity}
  \pcomment{editing notes, ARM 5/12/16}
\end{pcomments}

\pkeywords{
  variance
  expectation
  random_variable
  independence
  additivity
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem starts here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}  Prove the pairwise independent additivity of variance
\inbook{Theorem~\bref{thm:variance_additivity}}: If $R_1, R_2,
\dots, R_n$ are pairwise independent random variables, then
\begin{equation}\label{}
\variance{R_1 + R_2 + \cdots + R_n} = \variance{R_1} + \variance{R_2}
+ \cdots + \variance{R_n}.\tag{*}
\end{equation}

\hint Why is it OK to assume $\expect{R_i} = 0$?

\begin{solution}

\begin{proof}
It is OK to assume that $\expect{R_i} = 0$ for $i=1,\dots,n$, since we
could always replace $R_i$ by $\paren{R_i-\expect{R_i}}$ in
equation~(*).  This substitution preserves the independence
of the variables, and by Theorem~\bref{var+const}, does not change the
variances.  With this assumption, Lemma~\bref{alt:var} implies that
\[
\variance{R_i} = \expect{R_i^2}.
\]
Also $\expect{R_1 + R_2 + \cdots + R_n} = 0$, so likewise
\[
\variance{R_1+R_2+\cdots+R_n} = \expect{(R_1+R_2+\cdots+R_n)^2}.
\]
So we need only prove
\begin{equation}\label{E2R+Rn}
\expect{(R_1+R_2+\cdots+R_n)^2} = \expect{R_1^2} + \expect{R_2^2} +
\cdots + \expect{R_n^2}
\end{equation}
But expectations of independent variables multiply
\inbook{ (Theorem~\bref{th:prod})}, so
\begin{equation}\label{rrindij}
\expect{R_iR_j} = \expect{R_i}\expect{R_j} = 0 \cdot 0 = 0
\end{equation}
for $i \neq j$, since $R_i$ and $R_j$ are independent.
Now~\eqref{E2R+Rn} follows from linearity of expectation:
\begin{align*}
\expect{(R_1+R_2+\cdots+R_n)^2}
   & = \expect{\sum_{1\leq i,j \leq n} R_iR_j}\\
   & = \sum_{1\leq i,j \leq n} \expect{R_iR_j} & \text{linearity of $\expect{}$}\\
   & = \sum_{1 \leq i \leq n} \expect{R_i^2}
             + \sum_{1 \leq i \neq j \leq n} \expect{R_iR_j} &
             \text{(rearranging the sum)}\\
   & = \sum_{1 \leq i \leq n} \expect{R_i^2}
            + \sum_{1 \leq i \neq j \leq n} 0
             & \text{(by~\eqref{rrindij})}\\
   & =  \expect{R_1^2} + \expect{R_2^2} + \cdots + \expect{R_n^2}.
\end{align*}
\end{proof}

Notice that the proof rests only on equation~\ref{rrindij} that
expectations multiply pairwise: two variables whose expectations
multiply are called uncorrelated:
\begin{definition*}
Random variables $R$ and $S$ are \emph{uncorrelated} when
\[
\expect{R \cdot S} = \expect{R} \cdot \expect{S}.
\]
\end{definition*}
\inbook{By Theorem\bref{th:prod}}\inhandout{We know} Independent
variables are uncorrelated, but uncorrelated variables need not be
independent.  The simplest example is a random variable $X$ which
takes the values $+1$ and $-1$ with equal probability.  Now $X$ and
$X^2$ are obviously not independent but are uncorrelated.

\end{solution}

\end{problem}

\endinput
