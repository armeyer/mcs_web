\documentclass[12pt]{article}
\usepackage{light}

%\newcommand{\Ex}{\mathop{\textup{E}}\nolimits}
%\newcommand{\Var}{\mathop{\textup{Var}}\nolimits}
%\newcommand{\Cov}{\mathop{\textup{Cov}}\nolimits}
%\newcommand{\expect}[1]{\Ex\left[#1\right]}
%\newcommand{\expectsq}[1]{{\Ex}^2\left[#1\right]}
%\newcommand{\expcond}[2]{\expect{#1\mid#2}}
%\newcommand{\variance}[1]{\Var\left[#1\right]}
%\newcommand{\varsq}[1]{{\Var}^2\left[#1\right]}
%\newcommand{\covar}[1]{\Cov\left[#1\right]}
%\newcommand{\covariance}[2]{\Cov\left[#1,#2\right]}

%\hidesolutions
\showsolutions

\newcommand{\pdf}{\textsc{PDF}}
\newcommand{\cdf}{\textsc{CDF}}

\begin{document}

\recitation{25}{December 7, 2012}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\insolutions{
\section{Expected Value Rule for Functions of Random Variables}

In lecture, we have computed the expectation of a function of a random
variable without explicitly discussing the general rule. For example,
yesterday we saw that the expectation of the square of the roll of a
die is not equal to the square of the expectation of the roll. That
is, if $R$ is the outcome of a single roll of a die. Then $\ex{R^2}
\neq \ex{R}^2$. We will now explicitly present the rule for computing
the expection of a function of a random variable $R$.

\begin{mathrule*}[Expected Value for the Function of a Random Variable]\label{exfuncrv}
Let $R$ be a random variable, and let $f(R)$ be a function of
$R$. Then, the expected value of the random variable $f(R)$ is given
by
%
\[
\expect{f(R)} = \sum_{x\in Range(R)} f(x)\cdot \pr{R=x}
\]
\end{mathrule*}
}

\section{Properties of Variance}
In this problem we will study some properties of the variance and the
standard deviation of random variables.

\begin{itemize}

\item[a.] Show that for any random variable $R$, $\variance{R} = \expect{R^2} - \expectsq{R}$. 

\solution{Let $\mu = \expect{R}$.  Then
\begin{align*}
\variance{R} & =   \expect{(R - \expect{R})^2}
               & \text{(Definition of variance)}\\
        & = \expect{(R - \mu)^2} & \text{(def. of $\mu$)}\\
        & = \expect{R^2 - 2  \mu R + \mu^2} \\
        & = \expect{R^2} - 2 \mu \expect{R} + \mu^2 
                & \text{(linearity of expectation)}\\
        & = \expect{R^2} - 2 \mu^2 + \mu^2
              &  \text{(def. of $\mu$)}\\
        & = \expect{R^2} - \mu^2\\
        & = \expect{R^2} - \expectsq{R}.
                  &  \text{(def. of $\mu$)}
\end{align*}}

\item[b.] Show that for any random variable $R$ and constants $a$ and $b$, $\variance{a R + b} = a^2 \variance{R}.$ Conclude that the standard deviation of $aR + b$ is $a$ times the standard deviation of $R$.

\solution{We will transform the left side into the right side.  The
first step is to expand $\variance{a R + b}$ using the alternate
definition of variance.
\[
\variance{a R + b} =
\expect{(a R + b)^2} - \expectsq{a R + b}.
\]
We will work on the first term and then the second term.  For the first
term, note that by linearity of expectation,
\begin{equation}\label{R2}
\expect{(aR+b)^2} = \expect{a^2 R^2 + 2 a b R + b^2} =
a^2 \expect{R^2} + 2 a b \expect{R} + b^2.
\end{equation}
Similarly for the second term:
\begin{equation}\label{2R}
\expectsq{aR+b} = (a \expect{R} + b)^2 = a^2 \expectsq{R} + 2 a b
\expect{R} + b^2.
\end{equation}

Finally, we subtract the expanded second term from the first.
\begin{align*}
\variance{a R + b} & = \expect{(a R + b)^2} - \expectsq{a R + b}
      & \text{(previous part)}\\
  & = a^2 \expect{R^2} + 2 a b \expect{R} + b^2 -\\
  &\quad (a^2 \expectsq{R} + 2 a b \expect{R} + b^2)
     & \text{(by~(\ref{R2}) and~(\ref{2R}))}\\
  & = a^2 \expect{R^2} - a^2 \expectsq{R}\\
  & = a^2 (\expect{R^2} - \expectsq{R})\\
  & = a^2 \variance{R} 
     & \text{(previous part)}
\end{align*}
Since the standard deviation of a random variable is the square root of the variance, the standard deviation of $aR+b$ is $\sqrt{a^2 \variance{R}}$ which is just $a$ times the standard deviation of $R$.
}

\item[c.] Show that if $R_1$ and $R_2$ are independent random variables, then
\[
\variance{R_1 + R_2} = \variance{R_1} + \variance{R_2}.
\]

\solution{
We will transform the left side into the right side.  We begin by
applying the alternate definition of variance.
\[
\variance{R_1 + R_2} = \expect{(R_1 + R_2)^2} - \expectsq{R_1 + R_2}.
\]

We will work on the first term and then the second term separately.
For the first term, note\begin{eqnarray*}
\expect{(R_1+R_2)^2}
& = &   \expect{R_1^2 + 2 R_1 R_2 + R_2^2} \\
& = &   \expect{R_1^2} + \expect{2 R_1 R_2} + \expect{R_2^2} \\
& = &   \expect{R_1^2} + 2 \expect{R_1} \expect{R_2} + \expect{R_2^2}.
\end{eqnarray*}
First, we multiply out the squared expression.  The second step uses
linearity of expectation.  In the last step, we break the
expectation of the product $R_1 R_2$ into a product of expectations;
this is where we use the fact that $R_1$ and $R_2$ are independent.
Now we work on the second term.
\begin{eqnarray*}
\expectsq{R_1+R_2} & = & (\expect{R_1} + \expect{R_2})^2 \\
& = & \expectsq{R_1} + 2 \expect{R_1} \expect{R_2} + \expectsq{R_2}.
\end{eqnarray*}
The first step uses linearity of expectation, and in the second step
we multiply out the squared expression.  Now we subtract the
(expanded) second term from the first. Cancelling and rearranging
terms, we find that
\begin{eqnarray*}
\variance{R_1 + R_2} & = &   (\expect{R_1^2} - \expectsq{R_1}) +
(\expect{R_2^2}) - \expectsq{R_2}) \\
& = &   \variance{R_1} + \variance{R_2}.
\end{eqnarray*}
}

\item[d.] Give an example of random variables $R_1$ and $R_2$ for which 
\[
\variance{R_1 + R_2} \neq \variance{R_1} + \variance{R_2}.
\]

\solution{Suppose $R = R_1 = R_2$. If linearity of variance held, then
$\variance{R + R} = \variance{R} + \variance{R}$. However, by part b, 
$\variance{R+R} = \variance{2R} = 4\variance{R}$. This is only possible
if $\variance{R} = 0$. If, say, we choose $R$ to be the outcome of a fair coin
flip, $\variance{R} \neq 0$. In fact, any $R$ which holds at least $2$ distinct values each with positive probability will do.
}

\item[e.] Compute the variance and standard deviation of the Binomial distribution $H_{n,p}$ with parameters $n$ and $p$.

\solution{We know that $H_{n,p} = \sum_{k=1}^n I_k$ where the $I_k$ are mutually
independent 0-1-valued variables with $\pr{I_k=1}=p$. The variance of $I_k$ is $\expect{I_k^2} - \expect{I_k}^2 = \expect{I_k} - \expect{I_k}^2 = \expect{I_k}(1-\expect{I_k}) = p(1-p)$. Thus, by linearity of variance, we have
$\variance{H_{n,p}} = n \variance{I_k} = np(1-p).$ Thus, the standard deviation of $H_{n,p}$ is $\sqrt{np(1-p)}$.}

\item[f.] Let's say we have a random variable $T$ such that
  $T=\sum_{j=1}^n T_j$, where all of the $T_j$'s are mutually
  independent and take values in the range $[0,1]$. Prove that
  Var(T)$\leq$Ex(T). We'll use this result in lecture tomorrow. {\it
    Hint: Upper bound $\variance{T_j}$ with $\expect{T_j}$ using the
    definition of variance in part (a) and the rule for computing the
    expectation of a function of a random variable.}

\solution{

We know by linearity of variance for mutually-independent random
variables that

\begin{align*}
\variance{T} 
& = \variance{T_1+\ldots+T_n}\\
& = \variance{T_1} + \ldots + \variance{T_n}\\
\end{align*}

Now we evaluate the variance of $T_j$ for any $j$. Using the
definition of variance from part (a) above, we have

\[
\variance{T_j} = \expect{T_j^2} - \expect{T_j}^2 
\]

By the rule for computing the expectation of a function of a random
variable, we also know that

\[
\expect{T_j^2} = \sum_{x\in Range(T_j)} x^2\pr{T_j=x}
\]

Now we can use the fact that $T_j$ is in the range $[0,1]$ to say that
$x^2 \leq x$, and that therefore

\[
\sum_{x\in Range(T_j)} x^2\cdot \pr{T_j=x} \leq \sum_{x\in Range(T_j)} x\cdot\pr{T_j=x}
\]

Thus, $\expect{T_j^2}\leq\expect{T_j}$ and

\begin{align*}
\variance{T_j}
& \leq \expect{T_j} - \expect{T_j}^2\\
& \leq \expect{T_j}\\
\end{align*}

Since this holds for any $j$, we can now conclude that

\begin{align*}
\variance{T}
& = \variance{T_1} + \ldots + \variance{T_n}\\
& \leq \expect{T_1} + \ldots + \expect{T_n}\\
& = \expect{T}\\
\end{align*}
}

\end{itemize}

\end{document}
