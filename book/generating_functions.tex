\chapter{Generating Functions}\label{generating_function_chap}

\idx{Generating Functions} are one of the most surprising and useful
inventions in Discrete Mathematics.  Roughly speaking, generating
functions transform problems about \textit{sequences} into problems
about \textit{algebra}.  This is great because we've got piles of
algebraic rules.  Thanks to generating functions, we can reduce
problems about sequences to checking properties of algebraic
expressions.  This will allow us to use generating functions to
solve all sorts of counting problems.

Several flavors of generating functions such as \emph{ordinary},
\emph{exponential}, and \emph{Dirichlet} come up regularly in
combinatorial mathematics.  In addition, \emph{Z-transforms}, which
are closely related to ordinary generating functions, are important in
control theory and signal processing.  But ordinary generating
functions are enough to illustrate the power of the idea, so we'll
stick to them.  So from now on \emph{generating function} will mean
the ordinary kind, and we will offer a taste of this large subject by
showing how generating functions can be used to solve certain kinds of
counting problems and how they can be used to find simple formulas for
\emph{linear-recursive} functions.

\section{Infinite Series}
Informally, a generating function, $F(x)$, is an infinite series
\begin{equation}\label{def:Fxf_0}
F(x) = f_0 + f_1 x + f_2 x^2 + f_3 x^3 + \cdots.
\end{equation}
We use the notation $[x^n]F(x)$ for the coefficient of $x^n$ in the
generating function $F(x)$.  That is, $[x^n]F(x) \eqdef f_n$.

We can analyze the behavior of any sequence of numbers $f_0, f_1 \dots
f_n \dots$ by regarding the elements of the sequence as successive
coefficients of a generating function.  It turns out that properties
of complicated sequences that arise from counting, recursive
definition, and programming problems are easy to explain by treating
them as generating functions.

Generating functions can produce noteworthy insights even when the
sequence of coefficients is trivial.  For example, let $G(x)$ be the
generating function for the infinite sequence of ones $1,1,\dots$,
namely, the \idx{geometric series}.
\begin{equation}\label{def:geomseries}
G(x) \eqdef 1+x+x^2+\cdots+x^n+\cdots.
\end{equation}
We'll use typical generating function reasoning to derive a simple
formula for $G(x)$.  The approach is actually an easy version of the
\idx{perturbation method} of Section~\ref{sec:perturbation}.
Specifically,
\[
\begin{series}
       G(x) & = & 1 & + & x & + & x^2 & + & x^3 & + &\cdots & + & x^{n} & + &\cdots \cr
     -xG(x) & = &   & - & x & - & x^2 & - & x^3 & - &\cdots & - & x^{n} & - & \cdots \cr
     \hline\cr
G(x) -xG(x) & = & 1.
\end{series}
\]
Solving for~$G(x)$ gives 
\begin{equation}\label{eq:G1/1-x}
     \frac{1}{1 - x} = G(x) \eqdef \sum_{n=0}^\infty x^n.
\end{equation}
In other words,
\[
[x^n]\paren{\frac{1}{1 - x}} = 1
\]
Continuing with this approach yields a nice formula for
\begin{equation}\label{def:nonnegintseries}
N(x) \eqdef 1+2x+3x^2+\cdots+(n+1)x^n+\cdots.
\end{equation}
Specifically,
\[
\begin{series}
              N(x) & = & 1 & + & 2x & + & 3x^2 & + & 4x^3 & + &\cdots & + & (n+1)x^{n} & + &\cdots \cr
            -xN(x) & = &   & - & x & - & 2x^2 & - & 3x^3 & - &\cdots & - & nx^{n} & - & \cdots\cr
            \hline\cr
       N(x) -xN(x) & = & 1 & + & x & + & x^2 & + & x^3 & + &\cdots & + & x^{n} & + &\cdots\cr
                   & = & G(x).
\end{series}
\]
Solving for~$N(x)$ gives 
\begin{equation}\label{eq:N1/1-x2}
\frac{1}{(1-x)^2} = \frac{G(x)}{1 - x}  = N(x) \eqdef  \sum_{n=0}^\infty (n+1)x^n.
\end{equation}
On other words,
\[
[x^n]\paren{\frac{1}{(1-x)^2}} = n+1.
\]


\subsection{Never Mind Convergence}

The numerical values of $G(x)$ are undefined when $\abs{x} \geq 1$
because the geometric series diverges.  So equation~\eqref{eq:G1/1-x}
holds numerically only when $\abs{x} < 1$; likewise for
equation~\eqref{eq:N1/1-x2}.  But in the context of generating
functions, we regard infinite series as formal algebraic objects and
equations such as~\eqref{eq:G1/1-x} and~\eqref{eq:N1/1-x2} as symbolic
identities that hold for purely algebraic reasons.  In fact, good use
can be made of generating functions determined by infinite series that
don't converge \emph{anywhere} (other than zero).  We'll explain this
further in Section~\ref{sec:power_series} at the end of this chapter,
but for now, take it on faith that we needn't worry about convergence.

\section{Counting with Generating Functions}\label{sec:gf_counting}
Generating functions are particularly useful for representing and
counting the number of ways to select $n$ things.  For example, if
there are two flavors of donuts---chocolate and plain---let $d_n$
be the number of ways to select $n$ chocolate or plain flavored
donuts.  So $d_n =n+1$ because there are $n+1$ such donut selections,
namely, all chocolate, 1 plain and $n-1$ chocolate, 2 plain and
$n-2$ chocolate,\dots, all plain.  We define a generating function,
$D(x)$, for counting these donut selections by letting the
coefficient of $x^n$ be $d_n$.  So by equation~\eqref{eq:N1/1-x2}
\begin{equation}\label{2donutgen}
D(x) = \frac{1}{(1-x)^2}.
\end{equation}

\subsection{Apples and Bananas too}
More generally, suppose we have two kinds of things---say apples and
bananas---and some constraints on how many of each may be selected.
Say there are $a_n$ ways to select $n$ apples and $b_n$ ways to select
$n$ bananas.  So the generating function for counting apples would be
\[
A(x) \eqdef \sum_{n=0}^\infty a_nx^n,
\]
and for bananas would be
\[
B(x) \eqdef \sum_{n=0}^\infty b_nx^n.
\]

Now suppose apples come in baskets of 6, so there is no way to select
$1$ to $5$ apples, one way to select 6 apples, no way to select 7,
etc.  In other words,
\[
a_n = \begin{cases}
      1 & \text{if $n$ is a multiple of 6},\\
      0 & \text{otherwise}.
\end{cases}
\]
In this case we would have
\begin{align*}
A(x)
& = 1 + x^6 + x^{12} + \cdots + x^{6n} + \cdots\\
& = 1 + y + y^2 + \cdots +y^n + \cdots
       & \text{where $y = x^6$},\\
& = \frac{1}{1 - y} = \frac{1}{1 - x^6}.
\end{align*}
Let's also suppose there are two kinds of bananas---red and yellow.
Now $b_n = n+1$ by the same reasoning used to count selections of $n$
chocolate and plain donuts, and by~\eqref{2donutgen} we have
\[
B(x) = \frac{1}{(1-x)^2}.
\]

So how many ways are there to select a mix of $n$ apples and bananas?
First, we decide how many apples to select.  This can be any number
$k$ from 0 to $n$.  We can then select these apples in $a_k$ ways, by
definition.  This leaves $n-k$ bananas to be selected, which by
definition can be done in $b_{n-k}$ ways.  So the total number of ways
to select $k$ apples and $n-k$ bananas is $a_kb_{n-k}$.  This means
that the total number of ways to select some size $n$ mix of apples
and bananas is
\begin{equation}\label{a0bnconvolve}
a_0b_n + a_1b_{n-1} + a_2b_{n-1} + \cdots + a_nb_0.
\end{equation}

\subsection{Products of Generating Functions}
Now here's the cool connection between counting and generating
functions: expression~\eqref{a0bnconvolve} is equal to the coefficient
of $x^n$ in the product $A(x)B(x)$.

In other words, we're claiming that\index{Product Rule for generating functions}
\begin{rul*}[Product]
\begin{equation}\label{xnAxBx}
[x^n](A(x)\cdot B(x)) = a_0b_n + a_1b_{n-1} + a_2b_{n-1} + \cdots +
a_nb_0.
\end{equation}
\end{rul*}
To explain the generating function Product Rule, we can think about
evaluating the product $A(x) \cdot B(x)$ by using a table to identify
all the cross-terms from the product of the sums:
\[
\begin{array}{c|@{\quad}c@{\qquad}c@{\qquad}c@{\qquad}c@{\qquad}c}
      & b_0 x^0 & b_1 x^1 & b_2 x^2 & b_3 x^3 & \dots \\
\hline
\\
a_0 x^0 & a_0 b_0 x^0 & a_0 b_1 x^1 & a_0 b_2 x^2 & a_0 b_3 x^3 & \dots \\
\\
a_1 x^1 & a_1 b_0 x^1 & a_1 b_1 x^2 & a_1 b_2 x^3 & \dots \\
\\
a_2 x^2 & a_2 b_0 x^2 & a_2 b_1 x^3 & \dots \\
\\
a_3 x^3 & a_3 b_0 x^3 & \dots \\
\\
\vdots & \dots\\
\end{array}
\]
In this layout, all the terms involving the same power of $x$ lie on a
45-degree sloped diagonal.  So the index-$n$ diagonal contains all the
$x^n$-terms, and the coefficient of $x^n$ in the product $A(x)\cdot
B(x)$ is the sum of all the coefficients of the terms on this
diagonal, namely,~\eqref{a0bnconvolve}.  The sequence of coefficients
of the product $A(x)\cdot B(x)$ is called the \term{convolution} of
the sequences $(a_0, a_1, a_2, \dots)$ and $(b_0, b_1, b_2, \dots)$.
In addition to their algebraic role, convolutions of sequences play a
prominent role in signal processing and control theory.

This Product Rule provides the algebraic justification for the fact
that a geometric series equals $1/(1-x)$ regardless of convergence.
Namely, the constant $1$ describes the generating function
\[
1 = 1+ 0x+ 0x^2 + \cdots + 0x^n + \cdots.
\]
Likewise, the expression $1-x$ describes the generating function
\[
1-x = 1+ (-1)x+ 0x^2 + \cdots + 0x^n + \cdots.
\]
\iffalse
Since all but the first two coefficents for $1-x$ are zero, the Product Rule implies that
for any generating function $F(x)$, 
\begin{align*}
[x^0]((1-x)F(x)) & = f_0\\
[x^n]((1-x)F(x)) & = f_n-f_{n-1}m & for $n>0$,
\end{align*}
so in particular for geometric series $G(x)$, the only nonzero coefficent
of $(1-x)\cdot G(x)$ is the coefficient of $x^0$, which equals $g_0 = 1$.
\fi
So for the series $G(x)$ whose coefficients are all equal to 1, the
Product Rule implies in a purely formal way that
\[
(1-x)\cdot G(x) = 1+ 0x+ 0x^2 + \cdots + 0x^n + \cdots = 1.
\]
In other words, under the Product Rule, the geometric series $G(x)$ is
the multiplicative inverse, $1/(1-x)$, of $1-x$.

Similar reasoning justifies multiplying a generating function by a
constant term by term.  That is, a special case of the Product Rule is
the
\begin{rul*}[Constant Factor]
For any constant, $c$, and generating function, $F(x)$,
\begin{equation}\label{xncFcn}
[x^n](c \cdot F(x)) = c \cdot [x^n]F(x).
\end{equation}
\end{rul*}

\iffalse
Letting
\[
P(x) = (1-x) \cdot (1+x+x^2+\cdots+x^n+\cdots),
\]
the Product Rule gives
\begin{align*}
[x^0]P(x) & = 1 \cdot 1 = 1,\\
[x^{n+1}]P(x)& = 1\cdot 1+ (-1)\cdot 1 +0\cdot 1 +\cdots + 0\cdot 1 = 0.
\end{align*}
That is,
\[
P(x) = 1 + 0\cdot x +  0\cdot x^2 + \cdots = 1.
\]
\fi

\subsection{The Convolution Rule}

We can summarize the discussion above with the
\begin{mathrule*}[\idx{Convolution}]\label{convolution_rule}
Let $A(x)$ be the generating function for selecting items from a set
$\mathcal{A}$, and let $B(x)$ be the generating function for selecting
items from a set $\mathcal{B}$ disjoint from $\mathcal{A}$.  The
generating function for selecting items from the union $\mathcal{A} \cup
\mathcal{B}$ is the product $A(x) \cdot B(x)$.
\end{mathrule*}

The Rule depends on a precise definition of what ``selecting items
from the union $\mathcal{A} \cup \mathcal{B}$'' means.  Informally,
the idea is that the restrictions on the selection of items from sets
$\mathcal{A}$ and $\mathcal{B}$ carry over to selecting items from
$\mathcal{A} \cup \mathcal{B}$.\footnote{Formally, the Convolution
  Rule applies when there is a bijection between $n$-element
  selections from $\mathcal{A} \union \mathcal{B}$ and ordered pairs
  of selections from the sets $\mathcal{A}$ and $\mathcal{B}$
  containing a total of $n$ elements.  We think the informal statement
  is clear enough.}

\subsection{Counting Donuts with the Convolution Rule}

We can use the Convolution Rule to derive in another way the
generating function $D(x)$ for the number of ways to select chocolate
and plain donuts given in~\eqref{2donutgen}.  To begin, there is only
one way to select exactly $n$ chocolate donuts.  That means every
coefficient of the generating function for selecting $n$ chocolate
donuts equals one.  So the generating function for chocolate donut
selections is $1/(1-x)$; likewise for the generating function for
selecting only plain donuts.  Now by the Convolution Rule, the
generating function for the number of ways to select $n$ donuts when
both chocolate and plain flavors are available is
\[
D(x) = \frac{1}{1-x} \cdot \frac{1}{1-x} = \frac{1}{(1-x)^2}.
\]
So we have derived~\eqref{2donutgen} without appeal
to~\eqref{eq:N1/1-x2}.

Our application of the Convolution Rule for two flavors carries right
over to the general case of $k$ flovors, and we conclude that the
generating function for selections of donuts when $k$ flavors are
available is $1/(1-x)^k$.  We already derived the formula for the
number of ways to select a $n$ donuts when $k$ flavors are
available, namely, $\binom{n+(k-1)}{n}$ from
Corollary~\ref{cor:donut_binom}.  So we have
\begin{equation}\label{eq:donut_coefficient}
[x^n]\paren{\frac{1}{(1-x)^k}} = \binom{n+(k-1)}{n}.
\end{equation}


\subsubsection{Extracting Coefficients from Maclaurin's Theorem}

We've used a donut-counting argument to derive the coefficients of
$1/(1-x)^k$, but it's instructive to derive this coefficient
algebraically, which we can do using Maclaurin's Theorem:
\begin{theorem}[Maclaurin's Theorem]\label{thm:maclauren}
\[
f(x) = f(0) + f'(0) x + \frac{f''(0)}{2!} x^2 + \frac{f'''(0)}{3!} x^3 + \cdots
+ \frac{f^{(n)}(0)}{n!} x^n + \cdots.
\]
\end{theorem}

This theorem says that the $n$th coefficient of $1 / (1 - x)^k$ is
equal to its $n$th derivative evaluated at 0 and divided by $n!$.
Computing the $n$th derivative turns out not to be very difficult
\[
\frac{d^n}{d^n x} \frac{1}{(1-x)^k} = k (k+1) \cdots (k + n - 1)(1-x)^{-(k+n)}
\]
(see Problem~\ref{CP_nth_derivative_of_A}), so
\begin{align*}
[x^n]\paren{\frac{1}{(1-x)^k}}
  & = \paren{\frac{d^n}{d^n x} \frac{1}{(1-x)^k}}(0)\frac{1}{n!}\\
  & = \frac{k (k+1) \cdots (k + n - 1)(1-0)^{-(k+n)}}{n!}\\
  & = \binom{n+ (k-1)}{n}.
\end{align*}
On other words, instead of using the donut-counting
formula~\eqref{eq:donut_coefficient} to find the coefficients of
$x^n$, we could have used this algebraic argument and the Convolution
Rule to derive the donut-counting formula.

\subsection{The Binomial Theorem from the Convolution Rule}

The Convolution Rule also provides a new perspective on the Binomial
Theorem~\ref{thm:binomial}.  Here is how.  First, work with the
single-element set $\set{a_1}$.  The generating function for the
number of ways to select $n$ different elements from this set is simply $1 + x$:
we have 1 way to select zero elements, 1 way to select the one
element, and 0 ways to select more than one element.  Similarly, the
number of ways to select $n$ elements from any single-element set
$\set{a_i}$ has the same generating function $1 + x$.  Now by the
Convolution Rule, the generating function for choosing a subset of $n$
elements from the set $\set{a_1,a_2,\dots,a_m}$ is the product
$(1+x)^m$ of the generating functions for selecting from each of the
$m$ one-element sets.  Since we know that the number of ways to select
$n$ elements from a set of size $m$ is $\binom{m}{n}$, we conclude that
that
\[
[x^n](1+x)^m = \binom{m}{n},
\]
which is a restatement of the Binomial Theorem~\ref{thm:binomial}.

So we have proved the Binomial Theorem without having to analyze the
expansion of the expression $(1+x)^m$ into a sum of products.

These examples of counting donuts and deriving the binomial
coefficients illustrate where generating functions get their power:

\textbox{Generating functions can allow counting problems to be solved
  by algebraic manipulation, and conversely, they can allow algebraic
  identities to be derived by counting techniques.}

\iffalse
The
fact that the elements differ in the two cases is irrelevant.

Now here is the main trick: \textit{the generating function for
choosing elements from a union of disjoint sets is the product of the
generating functions for choosing from each set.}  We'll justify this
in a moment, but let's first look at an example.  According to this
principle, the generating function for the number of ways to select
$n$ elements from the $\set{a_1, a_2}$ is:
%
\[
\underbrace{(1 + x)}_{
\begin{array}{cc}
\text{gen func for} \\
\text{selecting an $a_1$}
\end{array}}
\cdot
\underbrace{(1 + x)}_{
\begin{array}{cc}
\text{gen func for} \\
\text{selecting an $a_2$}

\end{array}}
=
\underbrace{(1 + x)^2}_{
\begin{array}{cc}
\text{gen func for} \\
\text{selecting from} \\
\set{a_1, a_2}
\end{array}}
= 1 + 2x + x^2
\]
%
Sure enough, for the set $\set{a_1, a_2}$, we have 1 way to select
zero elements, 2 ways to select one element, 1 way to select two
elements, and 0 ways to select more than two elements.

Repeated application of this rule gives the generating function for
selecting $n$ items from a $k$-element set $\set{a_1, a_2, \dots,
a_k}$:
%
\[
\underbrace{(1 + x)}_{
\begin{array}{cc}
\mbox{gen func for} \\
\text{selecting an $a_1$}
\end{array}}
\cdot
\underbrace{(1 + x)}_{
\begin{array}{cc}
\mbox{gen func for} \\
\text{selecting an $a_2$}
\end{array}}
\cdots
\underbrace{(1 + x)}_{
\begin{array}{cc}
\mbox{gen func for} \\
\text{selecting an $a_k$}
\end{array}}
=
\underbrace{(1 + x)^k}_{
\begin{array}{cc}
\mbox{gen func for} \\
\text{selecting from} \\
\set{a_1, a_2, \dots, a_k}
\end{array}}
\]
%
This is the same generating function that we obtained by using the
Binomial Theorem.  But this time around we translated directly from
the counting problem to the generating function.
\fi

\begin{editingnotes}
Let
%
\[
G(x) \eqdef \frac{1}{(1-x)^k} = (1-x)^{-k}.
\]
%
Then we have:
%
\begin{align*}
G'(x) & = k (1-x)^{-(k+1)} \\
G''(x) & = k (k+1) (1-x)^{-(k+2)} \\
G'''(x) & = k (k+1) (k+2) (1-x)^{-(k+3)} \\
G^{(n)}(x) & = k (k+1) \cdots (k + n - 1)(1-x)^{-(k+n)}
\end{align*}
%
Thus, the coefficient of $x^n$ in the generating function is:
%
\begin{align*}
G^{(n)}(0) / n! & = \frac{k (k+1) \cdots (k + n - 1)}{n!} \\
                & = \frac{(k + n - 1)!}{(k - 1)! \ n!} \\
                & = \binom{n + k - 1}{n}.
\end{align*}

\end{editingnotes}

\subsection{An Absurd Counting Problem}
\label{sec:impossible_counting}

So far everything we've done with generating functions we could have
done another way.  But here is an absurd counting problem---really
over the top!  In how many ways can we fill a bag with $n$ fruits
subject to the following constraints?

\begin{itemize}
\item The number of apples must be even.
\item The number of bananas must be a multiple of 5.
\item There can be at most four oranges.
\item There can be at most one pear.
\end{itemize}

For example, there are 7 ways to form a bag with 6 fruits:
%
\[
\begin{array}{c|ccccccc}
\text{Apples}  & 6 & 4 & 4 & 2 & 2 & 0 & 0 \\
\text{Bananas} & 0 & 0 & 0 & 0 & 0 & 5 & 5 \\
\text{Oranges} & 0 & 2 & 1 & 4 & 3 & 1 & 0 \\
\text{Pears}   & 0 & 0 & 1 & 0 & 1 & 0 & 1
\end{array}
\]
These constraints are so complicated that getting a nice answer may
seem impossible.  But let's see what generating functions reveal.

First we'll construct a generating function for choosing apples.  We
can choose a set of 0 apples in one way, a set of 1 apple in zero
ways (since the number of apples must be even), a set of 2 apples in
one way, a set of 3 apples in zero ways, and so forth.  So, we have:
%
\[
A(x) = 1 + x^2 + x^4 + x^6 + \cdots = \frac{1}{1 - x^2}
\]
%
Similarly, the generating function for choosing bananas is:
%
\[
B(x) = 1 + x^5 + x^{10} + x^{15} + \cdots = \frac{1}{1 - x^5}
\]
Now, we can choose a set of 0 oranges in one way, a set of 1 orange in
one way, and so on.  However, we cannot choose more than four
oranges, so we have the generating function:
%
\[
O(x) = 1 + x + x^2 + x^3 + x^4 = \frac{1-x^5}{1-x}\, .
\]
Here the right hand expression is simply the
formula~\eqref{geometric-sum-n} for a finite geometric sum.  Finally,
we can choose only zero or one pear, so we have:
%
\[
P(x) = 1 + x
\]

The Convolution Rule says that the generating function for choosing
from among all four kinds of fruit is:
%
\begin{align*}
A(x) B(x) O(x) P(x)
    & = \frac{1}{1-x^2} \frac{1}{1-x^5} \frac{1-x^5}{1-x} (1 + x) \\
    & = \frac{1}{(1-x)^2} \\
    & = 1 + 2x + 3x^2 + 4 x^3 + \cdots
\end{align*}
%
Almost everything cancels!  We're left with $1 / (1-x)^2$, which we
found a power series for earlier: the coefficient of $x^n$ is simply
$n+1$.  Thus, the number of ways to form a bag of $n$ fruits is just
$n+1$.  This is consistent with the example we worked out, since there
were 7 different fruit bags containing 6 fruits.  \textit{Amazing!}

\begin{problems}
\practiceproblems
\pinput{TP_bouquet}
\pinput{TP_Generating_Functions_and_Sequences}

\classproblems
\pinput{CP_nth_derivative_of_A}
\pinput{CP_gen_func_sum_of_squares}

\homeworkproblems
\pinput{PS_gen_fcns_pennies_nickels_etc}
\pinput{PS_easy_impossible_count}

\end{problems}


\section{Partial Fractions}\label{sec:partial-fraction}

We got a simple solution to the ``impossible'' counting problem of
Section~\ref{sec:impossible_counting} because its generating function
simplified to the expression $1/(1-x)^2$, whose power series
coefficients we already knew.  Of course, you probably guessed that
this problem was contrived so the answer would work out neatly.  But
other problems may not be so neat.  To solve more general problems
using generating functions, we need ways to find power series
coefficients for generating functions given as formulas.  Maclaurin's
Theorem~\ref{thm:maclauren} is a very general method for finding
coefficients, but it only applies when formulas for repeated
derivatives can be found, which isn't often.  However, there is an
automatic way to find the power series coefficients for any formula
that is a quotient of polynomials: the method of \idx{partial
  fractions} from elementary calculus.

The partial fraction method is based on the fact that quotients of
polynomials can be expressed as sums of terms whose power series
coefficients have nice formulas.  For example when the denominator
polynomial has distinct nonzero roots, the method rests on
\begin{lemma}\label{lem:partial-fraction-distinct-roots}
  Let $p(x)$ be a polynomial of degree less than $n$ and let
  $\alpha_1, \dots, \alpha_n$ be distinct, nonzero numbers.  Then
  there are constants $c_1,\dots,c_n$ such that
\[
\frac{p(x)}{(1-\alpha_1 x)(1-\alpha_2 x)\cdots(1-\alpha_n x)} =
\frac{c_1}{1-\alpha_1 x} + \frac{c_2}{1-\alpha_2 x} + \cdots +
\frac{c_n}{1-\alpha_n x}.
\]
\end{lemma}

Let's illustrate the use of Lemma~\ref{lem:partial-fraction-distinct-roots}
by finding the power series coefficients for the function
\[
R(x) \eqdef \frac{x}{1 - x - x^2}.
\]
We can use the quadratic formula to find the roots $r_1,r_2$ of the denominator, $1 - x - x^2$.
\[
r_1 = \frac{-1 -\sqrt{5}}{2}, r_2 = \frac{-1 + \sqrt{5}}{2}.
\]
So
\[
1 - x - x^2 = (x - r_1)(x - r_2) = r_1r_2(1-x/r_1)(1-x/r_2).
\]
With a little algebra, we find that
\[
R(x) = \frac{x}{(1-\alpha_1 x)(1-\alpha_2 x)}
\]
where
\begin{align*}
\alpha_1 & = \frac{1 + \sqrt{5}}{2}\\
\alpha_2 & = \frac{1 - \sqrt{5}}{2}.
\end{align*}
Next we find $c_1$ and $c_2$ which satisfy:
\begin{equation}\label{x1xx2partial}
\frac{x}{(1-\alpha_1 x)(1-\alpha_2 x)} =
      \frac{c_1}{1 - \alpha_1 x} + \frac{c_2}{1 - \alpha_2 x}
\end{equation}
In general, we can do this by plugging in a couple of values for $x$
to generate two linear equations in $c_1$ and $c_2$ and then solve the
equations for $c_1$ and $c_2$.  A simpler approach in this case comes
from multiplying both sides of~\eqref{x1xx2partial} by the left hand
denominator to get
\[
x = c_1(1 - \alpha_2 x) + c_2(1 - \alpha_1 x).
\]
Now letting $x = 1/\alpha_2$ we obtain
\[
c_2 = \frac{1/\alpha_2}{1-\alpha_1/\alpha_2} = \frac{1}{\alpha_2 - \alpha_1} = -\frac{1}{\sqrt{5}},
\]
and similarly, letting $x = 1/\alpha_1$ we obtain
\[
c_1 =\frac{1}{\sqrt{5}}.
\]
\iffalse
\begin{gather*}
c_1 = \frac{1}{\alpha_1 - \alpha_2} = \frac{1}{\sqrt{5}} \\
c_2 = \frac{-1}{\alpha_1 - \alpha_2} = -\frac{1}{\sqrt{5}}
\end{gather*}
\fi
Plugging these values for $c_1,c_2$ into
equation~\eqref{x1xx2partial} finally gives the partial fraction
expansion
%
\[
R(x) = \frac{x}{1 - x - x^2} =
      \frac{1}{\sqrt{5}}
      \paren{\frac{1}{1 - \alpha_1 x} - \frac{1}{1 - \alpha_2 x}}
\]
%
Each term in the partial fractions expansion has a simple power series
given by the geometric sum formula:
%
\begin{align*}
\frac{1}{1 - \alpha_1 x} & = 1 + \alpha_1 x + \alpha_1^2 x^2 + \cdots \\
\frac{1}{1 - \alpha_2 x} & = 1 + \alpha_2 x + \alpha_2^2 x^2 + \cdots
\end{align*}
%
Substituting in these series gives a power series for the generating
function:
%
\[
R(x)  =  \frac{1}{\sqrt{5}}
      \paren{(1 + \alpha_1 x + \alpha_1^2 x^2 + \cdots) -
      (1 + \alpha_2 x + \alpha_2^2 x^2 + \cdots)},
\]
so
\begin{align}
 [x^n]R(x) & = \frac{\alpha_1^n - \alpha_2^n}{\sqrt{5}} \notag \\
           & = \frac{1}{\sqrt{5}}
      \paren{
      \paren{\frac{1 + \sqrt{5}}{2}}^n -
      \paren{\frac{1 - \sqrt{5}}{2}}^n}\label{fib-coeff}
\end{align}

\subsection{Partial Fractions with Repeated Roots}
Lemma~\ref{lem:partial-fraction-distinct-roots} generalizes to the
case when the denominator polynomial has a repeated nonzero root with
multiplicity $m$ by expanding the quotient into a sum a terms of the form
\[
\frac{c}{(1-\alpha x)^k}
\]
where $\alpha$ is the reciprocal of the root and $k \leq m$.
A formula for the coefficients of such a term follows
from the donut formula~\eqref{eq:donut_coefficient}.
\begin{equation}\label{c-alpha-k-coeff}
[x^n]\paren{\frac{c}{(1-\alpha x)^k}} = c \alpha^n \binom{n-(k-1)}{n}.
\end{equation}
When $\alpha = 1$, this follows from the donut
formula~\eqref{eq:donut_coefficient} and termwise multiplication by
the constant $c$.  The case for arbitrary $\alpha$ follows by
substituting $\alpha x$ for $x$ in the power series; this changes $x^n$ into
$(\alpha x)^n$ and so has the effect of multiplying the coefficient of
$x^n$ by $\alpha^n$.\footnote{In other words,
\[
[x^n]F(\alpha x) = \alpha^n \cdot [x^n]F(x).
\]}

\begin{problems}

\classproblems
\pinput{CP_bag_of_donuts}

\homeworkproblems
\pinput{PS_crazy_pet_lady}

\examproblems
\pinput{FP_boat_trip}
\end{problems}

\section{Solving Linear Recurrences}\label{sec:fibonacci}

\subsection{A Generating Function for the Fibonacci Numbers}

The Fibonacci numbers $f_0,f_1,\dots,f_n,\dots$ are defined recursively
as follows:
\begin{align*}
f_0 & \eqdef 0 \\
f_1 & \eqdef 1 \\
f_n & =\eqdef f_{n-1} + f_{n-2}
   & \text{(for $n \geq 2$)}.
\end{align*}
Generating functions will now allow us to derive an astonishing closed formula for $f_n$.

Let $F(x)$ be the generating function for the sequence of
Fibonacci numbers, that is,
\[
F(x) \eqdef f_0 + f_1 x + f_2 x^2 + \cdots f_n x^n + \cdots.
\]
Reasoning as we did at the start of this chapter to derive the formula
for a geometric series, we have
\[
\begin{array}{rcrcrcrcrcrcr}
F(x)     & = & f_0 & + & f_1 x & + & f_2 x^2 & + & \cdots & + &    f_n x^n + \cdots.\\
-xF(x)   & = &     & - & f_0 x & - & f_1 x^2 & - & \cdots & - & f_{n-1} x^n + \cdots.\\
-x^2F(x) & = &     &   &       & - & f_0 x^2 & - & \cdots & - & f_{n-2} x^n + \cdots.\\
\hline
F(x)(1-x-x^2)
         & = & f_0 & + & (f_1-f_0) x
                               & + &   0 x^2 & + & \cdots & + & 0 x^n + \cdots.\\
         & = &  0  & + & 1   x & + &   0 x^2 & = & x,
\end{array}
\]
so
\[
F(x) = \frac{x}{1 - x - x^2}.
\]
But $F(x)$ is the same as the function we used to illustrate the
partial fraction method for finding coefficients in
Section~\ref{sec:partial-fraction}.  So by equation~\eqref{fib-coeff},
we find that
\[
 f_n  = \frac{1}{\sqrt{5}}
      \paren{
      \paren{\frac{1 + \sqrt{5}}{2}}^n -
      \paren{\frac{1 - \sqrt{5}}{2}}^n}
\]
As a formula for Fibonacci numbers, this is astonishing and maybe
scary.  From the formula, it's not even obvious that its value is an
integer.  But the formula is very useful.  For example, it provides
(via the repeated squaring method) a much more efficient way to
compute Fibonacci numbers than crunching through the recurrence.  It
also clearly reveals the exponential growth of these numbers.

\subsection{The Towers of Hanoi}\label{hanoi-subsec}
According to legend, there is a temple in Hanoi with three posts and
64 gold disks of different sizes.  Each disk has a hole through the
center so that it fits on a post.  In the misty past, all the disks
were on the first post, with the largest on the bottom and the
smallest on top, as shown in Figure~\ref{fig:10A1}.

\begin{figure}

\graphic{Fig_10-1}

\caption{The initial configuration of the disks in the Towers of Hanoi
  problem.}

\label{fig:10A1}

\end{figure}

Monks in the temple have labored through the years since to move all
the disks to one of the other two posts according to the following
rules:
\begin{itemize}
\item The only permitted action is removing the top disk from one post
and dropping it onto another post.
\item A larger disk can never lie above a smaller disk on any post.
\end{itemize}
So, for example, picking up the whole stack of disks at once
and dropping them on another post is illegal.  That's good, because
the legend says that when the monks complete the puzzle, the world
will end!

To clarify the problem, suppose there were only 3 gold disks instead
of 64.  Then the puzzle could be solved in 7 steps as shown in
Figure~\ref{fig:10A2}.

\begin{figure}

\graphic{Fig_10-2}

\caption{The 7-step solution to the Towers of Hanoi problem when there
are $n = 3$~disks.}

\label{fig:10A2}

\end{figure}

The questions we must answer are, ``Given sufficient time, can the
monks succeed?''  If so, ``How long until the world ends?''  And, most
importantly, ``Will this happen before the final exam?''

\subsubsection{A Recursive Solution}

The Towers of Hanoi problem can be solved recursively.  As we describe
the procedure, we'll also analyze the minimum number, $t_n$, of steps
required to solve the $n$-disk problem.  For example, some
experimentation shows that $t_1 = 1$ and $t_2 = 3$.  The procedure
illustrated above uses 7 steps, which shows that $t_3$ is at most 7.

The recursive solution has three stages, which are described below and
illustrated in Figure~\ref{fig:10A3}.
For clarity, the largest disk is shaded in the figures.

\begin{editingnotes}
NEED NEW FIGURE indicating $n$ discs using vdots and with shading that shows.
\end{editingnotes}

\begin{figure}

\graphic{Fig_10-3}

\caption{A recursive solution to the Towers of Hanoi problem.}

\label{fig:10A3}

\end{figure}

\begin{description}

\item[Stage 1.]  Move the top $n-1$ disks from the first post to
  the second using the solution for $n - 1$ disks.  This can be done
  in $t_{n-1}$ steps.

\item[Stage 2.]  Move the largest disk from the first post to the
  third post.  This takes just 1 step.

\item[Stage 3.]  Move the $n-1$ disks from the second post to
  the third post, again using the solution for $n - 1$ disks.  This
  can also be done in $t_{n-1}$ steps.

\end{description}

This algorithm shows that $t_n$, the minimum number of steps required
to move $n$ disks to a different post, is at most $t_{n-1} + 1 +
t_{n-1} = 2 t_{n-1} + 1$.  We can use this fact to upper bound the
number of operations required to move towers of various heights:
\begin{align*}
t_3 & \leq 2 \cdot t_2 + 1 = 7 \\
t_4 & \leq 2 \cdot t_3 + 1 \leq 15
\end{align*}
Continuing in this way, we could eventually compute an upper bound on
$t_{64}$, the number of steps required to move 64 disks.  So this
algorithm answers our first question: given sufficient time, the monks
can finish their task and end the world.  This is a shame.  After all
that effort, they'd probably want to smack a few high-fives and go out
for burgers and ice cream, but nope---world's over.

\subsubsection{Finding a Recurrence}
We cannot yet compute the exact number of steps that the monks need
to move the 64 disks, only an upper bound.  Perhaps, having pondered
the problem since the beginning of time, the monks have devised a
better algorithm.

Lucky for us, there is no better algorithm. Here's why: at some step,
the monks must move the largest disk from the first post to a
different post.  For this to happen, the $n - 1$ smaller disks must all
be stacked out of the way on the only remaining post.  Arranging the $n
- 1$ smaller disks this way requires at least $t_{n-1}$ moves.  After
the largest disk is moved, at least another $t_{n-1}$ moves are
required to pile the $n - 1$ smaller disks on top.

This argument shows that the number of steps required is at least
$2t_{n-1} + 1$.  Since we gave an algorithm using exactly that number
of steps, we can now write an expression for $t_n$, the number of
moves required to complete the Towers of Hanoi problem with $n$ disks:
\begin{align*}
t_0 & = 0 \\
t_n & = 2t_{n-1} + 1 \text{\qquad (for $n \geq 1$)}.
\end{align*}

\subsubsection{Solving the Recurrence}
We can now find a formula for $t_n$ using generating functions.
Let $T(x)$ be the generating function for the $t_n$'s, that
is,
\[
T(x) \eqdef t_0 + t_1 x + t_2 x^2 + \cdots t_n x^n + \cdots.
\]
Reasoning as we did for the Fibonacci recurrence, we have
\[
\begin{array}{rcrcrcrcrcrcrcr}
T(x)     & = & t_0 & + &  t_1 x & + & \cdots & + &     t_n x^n + \cdots\\
-2xT(x)  & = &     & - & 2t_0 x & - & \cdots & - & 2t_{n-1} x^n + \cdots\\
-1/(1-x) & = &  -1 & -  &   1 x & - & \cdots & - &       1 x^n + \cdots\\
\hline
T(x)(1-2x) - 1/(1-x)
         & = & t_0 -1 & + & 0 x & + & \cdots & + &       0 x^n + \cdots\\
         & = & -1,
\end{array}
\]
so
\[
T(x)(1-2x) = \frac{1}{1-x} -1 = \frac{x}{1-x},
\]
and
\[
T(x) = \frac{x}{(1-2x)(1-x)}.
\]
Using partial fractions,
\[
 \frac{x}{(1-2x)(1-x)} = \frac{c_1}{1-2x} +\frac{c_2}{1-x}
\]
for some constants $c_1,c_2$.  Now multiplying both sides by the left
hand denominator gives
\[
 x = c_1(1-x) +c_2(1-2x).
\]
Substituting $1/2$ for $x$ yields $c_1 = 1$ and substituting $1$ for
$x$ yields $c_2 = - 1$, which gives
\[
T(x) = \frac{1}{1-2x} - \frac{1}{1-x}.
\]
Finally we can read off the simple formula for the numbers of steps
needed to move a stack of $n$ disks:
\[
t_n = [x^n]T(x) = [x^n]\paren{\frac{1}{1-2x}} - [x^n]\paren{\frac{1}{1-x}} = 2^n - 1.
\]

\subsection{Solving General Linear Recurrences}
An equation of the form
\begin{equation}\label{fnc1c2}
f(n) = c_1 f(n-1) + c_2 f(n-2) +  \cdots + c_{d} f(n-d) + h(n)
\end{equation}
for constants $c_i \in \complexes$ is called a \term{degree $d$ linear
  recurrence} with inhomogeneous term $h(n)$.

The methods above extend straightforwardly to solving linear
recurrences with a large class of inhomogeneous terms.  In particular,
when the inhomogeneous term itself has a generating function that can
be expressed as a quotient of polynomials, the approach used above to
derive generating functions for the Fibonacci and Tower of Hanoi
examples carries over to yield a quotient of polynomials that defines
the generating function $f(0)+f(1)x+f(2)x^2+\cdots$.  Then partial
fractions can be used to find a formula for $f(n)$ that is a linear
combination of terms of the form $n^k\alpha^n$ where $k$ is a
nonnegative integer $\leq d$ and $\alpha$ is the reciprocal of a
root of the denominator polynomial.  For example, see
Problems~\ref{CP_Fibonacci_and_bunnies},~\ref{FP_linear_recur},~\ref{MQ_gen_2} and
\ref{CP_towers_of_Sheboygan}.

\begin{problems}
\practiceproblems
\pinput{TP_Generating_function_of_a_recurrence}

\classproblems
\pinput{CP_Fibonacci_and_bunnies}
\pinput{CP_towers_of_Sheboygan}

\homeworkproblems
\pinput{PS_gen_fcn_quotient_polynomials}
\pinput{PS_Catalan_numbers_meyer_version}

\examproblems
\pinput{MQ_gen_2}
\pinput{FP_linear_recur}

\end{problems}

\section{Formal Power Series}\label{sec:power_series}

\subsection{Divergent Generating Functions}
Let $F(x)$ be the generating function for $n!$, that is,
\[
F(x) \eqdef 1 + 1x + 2x^2 + \cdots + n! x^n + \cdots.
\]
Because $x^n = o(n!)$ for all $x \neq 0$, this generating function
converges only at $x=0$.\footnote{This section is based on an example
  from ``Use of everywhere divergent generating function,''
  \texttt{math}\textbf{\texttt{overflow}}, response 8,147 by Aaron
  Meyerowitz, Nov. 12, 2010.}

Next, let $H(x)$ be the generating function for $n
\cdot n!$, that is, 
\[
H(x) \eqdef 0 + 1x + 4x^2 + \cdots + n\cdot n! x^n + \cdots.
\]
Again, $H(x)$ converges only for $x = 0$, so $H(x)$ and $F(x)$
  describe the same, trivial, partial function on the reals.

On the other hand, $F(x)$ and $H(x)$ have different coefficients for
  all powers of $x$ greater than 1, and we can usefully distinguish
  them as formal, symbolic objects.

To illustrate this, note than by subtracting 1 from $F(x)$ and then
dividing each of the remaining terms by $x$, we get a series where the
coefficient if $x^n$ is $(n+1)!$.  That is
\begin{equation}\label{xnFx1}
[x^n]\paren{\frac{F(x) - 1}{x}} = (n+1)!\, .
\end{equation}

Now a little further formal reasoning about $F(x)$ and $H(x)$ will
allow us to deduce the following identity for $n!$:\footnote{A
combinatorial proof of~\eqref{n!sumagain} is given in
Problem~\ref{CP_factorial_sum}}
\begin{equation}\label{n!sumagain}
n! = 1 + \sum_{i=1}^{n} (i-1) \cdot (i-1)!
\end{equation}

To prove this identity, note that from~\eqref{xnFx1}, we have
\[
[x^n]H(x) \eqdef n \cdot n! = (n+1)! - n! = [x^n]\paren{\frac{F(x) - 1}{x}} - [x^n]F(x).
\]
In other words,
\begin{equation}\label{HF-1}
H(x) = \frac{F(x) - 1}{x} - F(x),
\end{equation}
Solving~\eqref{HF-1} for $F(x)$, we get
\begin{equation}\label{FxH1}
F(x) = \frac{xH(x)+1}{1-x}.
\end{equation}
But $[x^n](xH(x)+1)$ is $(n-1) \cdot (n-1)!$ for $n \geq 1$ and is 1
for $n=0$, so by the convolution formula,
\[
[x^n]\paren{\frac{xH(x)+1}{1-x}} = 1 + \sum_{i=1}^{n} (i-1) \cdot (i-1)! \, .
\]
The identity~\eqref{n!sumagain} now follows immediately
from~\eqref{FxH1}.

\subsection{The Ring of Power Series}
So why don't we have to worry about series whose radius of convergence
is zero, and how do we justify the kind of manipulation in the
previous section to derive the formula~\eqref{FxH1}?  The answer comes
from thinking abstractly about infinite sequences of numbers and
operations that can be performed on them.

For example, one basic operation combining two infinite sequences is
adding them coordinatewise.  That is, if we let
\begin{align*}
G & \eqdef (g_0,g_1,g_2,\dots),\\
H & \eqdef (h_0,h_1,h_2,\dots),
\end{align*}
then we can define the sequence sum, $\oplus$, by the rule:
\[
G \oplus H \eqdef (g_0+h+0, g_1+h_1, \dots, g_n+h_n, \dots).
\]
Another basic operation is sequence multiplication, $\otimes$, defined
by the convolution rule (\emph{not} coordinatewise):
\[
G \otimes H \eqdef \paren{g_0+h_0, g_0h_1+g_1h_0, \dots, \sum_{i=0}^n g_ih_{n-i}, \dots}.
\]
These operations on infinite sequences have lots of nice properties.
For example, it's easy to check that sequence addition and
multiplication are commutative:
\begin{align*}
G \oplus H & = H \oplus G,\\
G \otimes H & = H \otimes G.
\end{align*}
If we let
\begin{align*}
Z & \eqdef (0,0,0,\dots),\\
I & \eqdef (1,0,0,\dots,0,\dots),
\end{align*}
then it's equally easy to check that $Z$ acts like a zero for
sequences and $I$ acts like the number one:
\begin{align}
Z \oplus G & = G,\notag\\
Z \otimes G & = Z,\label{ZoGZ}\\
I \otimes G & = G.\notag
\end{align}
Now if we define
\[
-G \eqdef (-g_0, -g_1, -g_2,\dots)
\]
then
\[
G \oplus (-G) = Z.
\]
In fact, the operations $\oplus$ and $\otimes$ satisfy all the
\emph{\idx{commutative ring}} axioms described in
Section~\ref{subsec:ringZn}.  The set of infinite sequences of
numbers together with these operations is called the \term{ring of
  formal power series} over these numbers.\footnote{The elements in
  the sequences may be the real numbers, complex numbers, or, more
  generally, may be the elements from any given commutative ring.}

\begin{editingnotes}
We can also define a ``derivative'' operator, $D$, and
``anti-derivative'' operator, $A$:
\begin{align*}
D(G) & \eqdef (g_1, 2g_2, \dots, ng_{n},\dots)\, .
A(G) & \eqdef (0, g_0, g_1/2, \dots, \frac{g_{n-1}}{n}, \dots)\,
\end{align*}
and verify that
\begin{align*}
D(A(G)) & = G\\
A(D(G)) & = G - g_0.
\end{align*}
\end{editingnotes}

A sequence $H$ is the \term{reciprocal} of a sequence $G$ when
\[
G \otimes H = I.
\]
A reciprocal of $G$ is also called a \term{multiplicative inverse} or
simply an ``inverse'' of $G$.  The ring axioms imply that if there is
a reciprocal, it is unique (see Problem~\ref{PS_ring_theory}), so the
suggestive notation $1/G$ can be used unambiguously to denote this
reciprocal, if it exists.  For example, letting
\begin{align*}
J & \eqdef (1,-1,0,0,\dots,0,\dots)\\
K & \eqdef (1,-1,0,0,\dots,0,\dots)
\end{align*}
the definition of $\otimes$ implies that $K = 1/J$.

In the ring of formal power series, equation~\eqref{ZoGZ} implies that
the zero sequence $Z$ has no inverse, so $1/Z$ is undefined---just as
the expression 1/0 is undefined over the real numbers or the ring
$\Zmod{n}$ of Section~\ref{subsec:ringZn}.  In general, a series has
an inverse iff its initial element is nonzero (see
Problem~\ref{CP_series_inverse}).

Now we can explain the proper way to understand a generating function
definition
\[
G(x) \eqdef \sum_{n=0}^\infty g_n x^n.
\]
It simply means that $G(x)$ really refers to its infinite sequence of
coefficients $(g_0,g_1,\dots)$ in the ring of formal power series.  The
simple expression, $x$, can be understood as referring to the sequence
\[
X \eqdef (0,1,0,0,\dots,0,\dots).
\]
Likewise, $1-x$ abbreviates the sequence $J$ above, and the familiar
equation
\begin{equation}\label{inverse1-x}
\frac{1}{1-x} = 1 + x + x^2 + x^3 + \cdots
\end{equation}
can be understood as a way of restating the assertion that $K$ is
$1/J$.  In other words, the powers of the variable $x$ just serve as a
place holders---and as reminders of the definition of convolution.
The equation~\eqref{inverse1-x} has nothing to do with the values of
$x$ or the convergence of the series.  Rather, it is stating a
property that holds in the ring of formal power series.  The reasoning
about the divergent series in the previous section is completely
justified as properties of formal power series.

\begin{problems}
\practiceproblems
\pinput{TP_I_series}
\pinput{TP_X_series}

\classproblems
\pinput{CP_series_inverse}
\end{problems}

\section{References}
\cite{Wilf1990}, \cite{GrahamKP1994}.
\endinput
