\chapter{First-Order Logic}\label{logic_chap}

\newcommand{\solves}{\text{Solves}}
\newcommand{\probs}{\text{Probs}}
\newcommand{\even}{\text{Evens}}
\newcommand{\primes}{\text{Primes}}

\section{Quantifiers}

There are a couple of assertions commonly made about a predicate: that it
is \emph{sometimes} true and that it is \emph{always} true.  For
example, the predicate
%
\[
\text{``$x^2 \geq 0$''}
\]
%
is always true when $x$ is a real number.  On the other hand, the
predicate
%
\[
\text{``$5x^2 - 7 = 0$''}
\]
%
is only sometimes true; specifically, when $x = \pm \sqrt{7/5}$.

There are several ways to express the notions of ``always true'' and
``sometimes true'' in English.  The table below gives some general
formats on the left and specific examples using those formats on the
right.  You can expect to see such phrases hundreds of times in
mathematical writing!
%
\begin{center}
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{Always True}} \\[1ex]
For all $n$, $P(n)$ is true. & For all $x \in \reals$, $x^2 \geq 0$. \\
$P(n)$ is true for every $n$. & $x^2 \geq 0$ for every $x \in \reals$. \\[2ex]
\multicolumn{2}{c}{\textbf{Sometimes True}} \\[1ex]
There exists an $n$ such that $P(n)$ is true. & There exists an $x \in \reals$ such that $5x^2 - 7 = 0$.\\
$P(n)$ is true for some $n$. & $5x^2 - 7 = 0$ for some $x \in \reals$.\\
$P(n)$ is true for at least one $n$. & $5x^2-7=0$ for at least one $x \in \reals$.
\end{tabular}
\end{center}

All these sentences quantify how often the predicate is true.
Specifically, an assertion that a predicate is always true is called a
\term{universal} quantification, and an assertion that a predicate is
sometimes true is an \term{existential} quantification.  Sometimes the
English sentences are unclear with respect to quantification:
%
\begin{center}
  ``If you can solve any problem we come up with, then you get an \emph{A}
  for the course.''
\end{center}
%
The phrase ``you can solve any problem we can come up with'' could
reasonably be interpreted as either a universal or existential
quantification:
%
\begin{quote}
``you can solve \emph{every} problem we come up with,''
\end{quote}
or maybe
\begin{quote}
``you can solve \emph{at least one} problem we come up with.''
\end{quote}
%
In any case, notice that this quantified phrase appears inside a
larger if-then statement.  This is quite normal; quantified statements
are themselves propositions and can be combined with and, or, implies,
etc., just like any other proposition.

\subsection{More Cryptic Notation}

There are symbols to represent universal and existential
quantification, just as there are symbols for ``and'' ($\wedge$),
``implies'' ($\implies$), and so forth.  In particular, to say that a
predicate, $P$, is true for all values of $x$ in some set, $D$, one
writes:
%
\[
\forall x \in D.\; P(x)
\]
%
The symbol $\forall$ is read ``for all'', so this whole expression is
read ``for all $x$ in $D$, $P(x)$ is true''.  To say that a predicate
$P(x)$ is true for at least one value of $x$ in $D$, one writes:
%
\[
\exists x \in D.\; P(x)
\]
%
The backward-E, \term{$\exists$}, is read ``there exists''.  So this
expression would be read, ``There exists an $x$ in $D$ such that $P(x)$ is
true.''  The symbols $\forall$ and $\exists$ are always followed by a
variable ---usually with an indication of the set the variable ranges over
---and then a predicate, as in the two examples above.

As an example, let $\probs$ be the set of problems we come up with,
$\solves(x)$ be the predicate ``You can solve problem $x$'', and $G$ be
the proposition, ``You get an \emph{A} for the course.''  Then the two
different interpretations of
%
\begin{quote}
  ``If you can solve any problem we come up with, then you get an \emph{A}
  for the course.''
\end{quote}
%
can be written as follows:
%
\[
(\forall x \in \probs.\; \solves(x)) \QIMP G,
\]
or maybe
\[
(\exists x \in \probs.\; \solves(x)) \QIMP G.
\]

\subsection{Mixing Quantifiers}

Many mathematical statements involve several quantifiers.  For
example, \term{Goldbach's Conjecture} states:
%
\begin{center}
``Every even integer greater than 2 is the sum of two primes.''
\end{center}
%
Let's write this more verbosely to make the use of quantification
clearer:
%
\begin{quote}
For every even integer $n$ greater than 2,
there exist primes $p$ and $q$ such that $n = p + q$.
\end{quote}
%
Let $\even$ be the set of even integers greater than 2, and let $\primes$ be the
set of primes.  Then we can write Goldbach's Conjecture in logic
notation as follows:
%
\[
\underbrace{\forall n \in \even}_{\substack
    {\text{for every even} \\
     \text{integer $n > 2$}}}
\
\underbrace{\exists p \in \primes\ \exists q \in \primes.}_{\substack
    {\text{there exist primes} \\
     \text{$p$ and $q$ such that}}}
\ n = p + q.
\]

\subsection{Order of Quantifiers}

Swapping the order of different kinds of quantifiers (existential or
universal) usually changes the meaning of a proposition.  For example,
let's return to one of our initial, confusing statements:
\begin{center}
``Every American has a dream.''
\end{center}

This sentence is ambiguous because the order of quantifiers is
unclear.  Let $A$ be the set of Americans, let $D$ be the set of
dreams, and define the predicate $H(a, d)$ to be ``American $a$ has
dream $d$.''.  Now the sentence could mean there is a single dream
that every American shares:
\[
\exists\, d \in D\; \forall a \in A.\; H(a, d)
\]
For example, it might be that every American shares the dream of owning
their own home.

Or it could mean that every American has a personal dream:
\[
\forall a \in A\; \exists\, d \in D.\; H(a, d)
\]
For example, some Americans may dream of a peaceful retirement, while
others dream of continuing practicing their profession as long as they
live, and still others may dream of being so rich they needn't think at
all about work.

Swapping quantifiers in \idx{Goldbach's Conjecture} creates a patently false
statement that every even number $\geq 2$ is the sum of \emph{the same}
two primes:
\[
\underbrace{\exists\, p \in \primes\ \exists\, q \in \primes}_{\substack
    {\text{there exist primes} \\
     \text{$p$ and $q$ such that}}}
\
\underbrace{\forall n \in \even.}_{\substack
    {\text{for every even} \\
     \text{integer $n > 2$}}}
\ n = p + q.
\]

\subsubsection{Variables over One Domain}
When all the variables in a formula are understood to take values from the
same nonempty set, $D$, it's conventional to omit mention of $D$.  For
example, instead of $\forall x \in D\; \exists y \in D.\; Q(x,y)$ we'd
write $\forall x \exists y.\; Q(x,y)$.  The unnamed nonempty set that $x$
and $y$ range over is called the \term{domain of discourse}, or just plain
\term{domain}, of the formula.

It's easy to arrange for all the variables to range over one domain.  For
example, \idx{Goldbach's Conjecture} could be expressed with all variables
ranging over the domain $\naturals$ as
\[
\forall n.\; n \in \even \QIMP (\exists\, p \exists\, q.\; p \in \primes \land
q \in \primes \land n = p + q).
\]

\subsection{Negating Quantifiers}

There is a simple relationship between the two kinds of quantifiers.  The
following two sentences mean the same thing:
%
\begin{quote}

It is not the case that everyone likes to snowboard.

There exists someone who does not like to snowboard.

\end{quote}
%
In terms of logic notation, this follows from a general property of
predicate formulas:
%
\[
\QNOT \forall x.\; P(x)
\hspace{0.1in} \text{is equivalent to} \hspace{0.1in}
\exists x.\; \QNOT P(x).
\]
%
Similarly, these sentences mean the same thing:
%
\begin{quote}
There does not exist anyone who likes skiing over magma.

Everyone dislikes skiing over magma.
\end{quote}
%
We can express the equivalence in logic notation this way:
%
\begin{equation}\label{nE}
(\QNOT \exists x.\; P(x))  \QIFF  \forall x.\; \QNOT P(x).
\end{equation}
%
The general principle is that \emph{moving a ``not'' across a
quantifier changes the kind of quantifier.}

\iffalse Logicians have worked very hard to define strict rules for the
use of logic notation so that ideas can be expressed with absolute rigor.
It's all quite charming and clever.  However, the sad irony is that
applied mathematicans usually use their beloved notation as a crude
shorthand, breaking the rules and abusing the notation willy-nilly ---sort
of like pounding nails with fine china.  \fi

\subsection{Validity}

A propositional formula is called \term{valid} when it evaluates to \true\
no matter what truth values are assigned to the individual propositional
variables.  For example, the propositional version of the \idx{Distributive Law}
is that $P \QAND (Q \QOR R)$ is equivalent to $(P \QAND Q) \QOR (P \QAND
R)$.  This is the same as saying that
\[
[P \QAND (Q \QOR R)] \QIFF [(P \QAND Q) \QOR (P \QAND R)]
\]
is valid.

The same idea extends to predicate formulas, but to be valid, a
formula now must evaluate to true no matter what values its variables
may take over any unspecified domain, and no matter what
interpretation a predicate variable may be given.  For example, we
already observed that the rule for negating a quantifier is captured
by the valid assertion~\eqref{nE}.

Another useful example of a valid assertion is
\begin{equation}\label{eaimpliesae}
\exists x \forall y.\; P(x,y) \QIMP \forall y \exists x.\; P(x,y).
\end{equation}

Here's an explanation why this is valid:

\begin{quote}
Let $D$ be the domain for the variables and $P_0$ be some
\idx{binary predicate}\footnote{That is, a predicate that depends on two variables.}
on $D$.  We need to show that if
\begin{equation}\label{exayp0}
\exists x \in D\; \forall y \in D.\; P_0(x,y)
\end{equation}
holds under this interpretation, then so does
\begin{equation}\label{ayexp0}
\forall y \in D\; \exists x \in D.\; P_0(x,y).
\end{equation}
So suppose~\eqref{exayp0} is true.  Then by definition of $\exists$, this
means that some element $d_0 \in D$ has the property that
\[
\forall y \in D.\, P_0(d_0, y).
\]
By definition of $\forall$, this means that
\[
P_0(d_0,d)
\]
is true for all $d \in D$.  So given any $d \in D$, there is an element in
$D$, namely, $d_0$, such that $P_0(d_0,d)$ is true.  But that's exactly
what~\eqref{ayexp0} means, so we've proved that~\eqref{ayexp0} holds under
this interpretation, as required.
\end{quote}

We hope this is helpful as an explanation, but we don't really want to
call it a ``proof.''  The problem is that with something as basic
as~\eqref{eaimpliesae}, it's hard to see what more elementary axioms are
ok to use in proving it.  What the explanation above did was translate the
logical formula~\eqref{eaimpliesae} into English and then appeal to the
meaning, in English, of ``for all'' and ``there exists'' as justification.
So this wasn't a proof, just an explanation that once you understand
what~\eqref{eaimpliesae} means, it becomes obvious.

In contrast to~\eqref{eaimpliesae}, the formula
\begin{equation}\label{aenotimplyea}
\forall y \exists x.\; P(x,y) \QIMP \exists x \forall y.\; P(x,y).
\end{equation}
is \emph{not} valid.  We can prove this just by describing an
interpretation where the hypothesis, $\forall y \exists x.\; P(x,y)$, is
true but the conclusion, $\exists x \forall y.\; P(x,y)$, is not true.
For example, let the domain be the integers and $P(x,y)$ mean $x > y$.
Then the hypothesis would be true because, given a value, $n$, for $y$ we
could choose the value of $x$ to be $n+1$, for example.  But under this
interpretation the conclusion asserts that there is an integer that is
bigger than all integers, which is certainly false.  An interpretation
like this which falsifies an assertion is called a \term{counter model} to
the assertion.

\begin{problems}
%\practiceproblems
%\pinput{}
\classproblems
\pinput{CP_logic_news_network}
\pinput{CP_assertions_about_binary_strings}
\pinput{CP_domain_of_discourse}
\pinput{CP_counter_model}
\homeworkproblems
\pinput{PS_express_in_predicate_form}
\pinput{PS_emailed_exactly_2_others}
\end{problems}

\hyperdef{logic}{sets}{\section{The Logic of Sets}}

\subsection{\idx{Russell's Paradox}}

Reasoning naively about sets turns out to be risky.  In fact, one of the
earliest attempts to come up with precise axioms for sets by a late
nineteenth century logican named Gotlob \term{Frege} was shot down by a three
line argument known as \emph{Russell's Paradox}:\footnote{Bertrand \term{Russell}
  was a Mathematician/Logician at Cambridge University at the turn of the
  Twentieth Century.  He reported that when he felt too old to do
  Mathematics, he began to study and write about Philosophy, and when he
  was no longer smart enough to do Philosophy, he began writing about
  Politics.  He was jailed as a conscientious objector during World War I.
  For his extensive philosophical and political writing, he won a Nobel
  Prize for Literature.}  This was an astonishing blow to efforts to
provide an axiomatic foundation for Mathematics.

\textbox{
\begin{quote}
Let $S$ be a variable ranging over all sets, and define
\[W \eqdef \set{S \suchthat S \not\in S}.\]
So by definition,
\[S \in W  \mbox{  iff  } S \not\in S,\]
for every set $S$.  In particular, we can let $S$ be $W$, and obtain
the contradictory result that
\[W \in W  \mbox{  iff  } W \not\in W.\]
\end{quote}}

A way out of the paradox was clear to Russell and others at the time:
\emph{it's unjustified to assume that $W$ is a set}.  So the step in the
proof where we let $S$ be $W$ has no justification, because $S$ ranges
over sets, and $W$ may not be a set.  In fact, the paradox implies that
$W$ had better not be a set!

But denying that $W$ is a set means we must \emph{reject} the very natural
axiom that every mathematically well-defined collection of elements is
actually a set.  So the problem faced by Frege, Russell and their
colleagues was how to specify \emph{which} well-defined collections are
sets.  Russell and his fellow Cambridge University colleague Whitehead
immediately went to work on this problem.  They spent a dozen years
developing a huge new axiom system in an even huger monograph called
\emph{Principia Mathematica}.


\subsection{The ZFC Axioms for Sets}
It's generally agreed that, using some simple logical deduction rules,
essentially all of Mathematics can be derived from some axioms about sets
called the Axioms of \idx{Zermelo-Frankel Set Theory} with Choice (\idx{ZFC}).

We're \emph{not} going to be working with these axioms in this course,
but we thought you might like to see them --and while you're at it, get
some practice reading quantified formulas:
%

\begin{description}

\item[\term{Extensionality}.] Two sets are equal if they have the same members.
In formal logical notation, this would be stated as:
\[
(\forall z.\; (z \in x \QIFF z \in y)) \QIMPLIES x = y.
\]


\item[\term{Pairing}.] For any two sets $x$ and $y$, there is a set,
     $\set{x,y}$, with $x$ and $y$ as its only elements:
\[
\forall x,y.\; \exists u.\; \forall z.\;
[z \in u \QIFF (z = x \QOR z = y)]
\]

\item[\index{Union axiom}Union.] The union, $u$, of a collection, $z$, of sets is also a set:
\[
\forall z.\, \exists u \forall x.\; (\exists y.\; x \in y \QAND y \in z) \QIFF x \in u.
\]

\item[\index{Infinity axiom}Infinity.]  There is an infinite set.
  Specifically, there is a nonempty set, $x$, such that for any set $y \in
  x$, the set $\set{y}$ is also a member of $x$.

\iffalse
\item[Subset.] Given any set, $x$, and any proposition $P(y)$, there is a
  set containing precisely those elements $y \in x$ for which $P(y)$ holds.
\fi

\item[\index{Power Set axiom}Power Set.]  All the subsets of a set form another set:
\[
\forall x.\; \exists p.\; \forall u.\: u \subseteq x \QIFF u \in p.
\]

\item[\index{Replacement axiom}Replacement.]  Suppose a formula, $\phi$,
  of set theory defines the graph of a function, that is,
\[
\forall x, y, z.\, [\phi(x,y) \QAND \phi(x,z)] \QIMPLIES y = z.
\]
Then the image of any set, $s$, under that function is also a set, $t$.  Namely,
\[
\forall s\, \exists t\, \forall y.\, [\exists x.\, \phi(x,y) \QIFF y \in t].
\]


\item[\term{Foundation}.] 
There cannot be an infinite sequence
\[
\cdots \in x_n \in \cdots \in x_1 \in x_0
\]
of sets each of which is a member of the previous one.  This is equivalent
to saying every nonempty set has a ``member-minimal'' element.  Namely, define
\[
\text{member-minimal}(m, x) \eqdef [m \in x \QAND \forall y \in x.\, y \notin m].
\]
Then the Foundation axiom is
\[
\forall x.\ x \neq \emptyset\ \QIMPLIES\ \exists m.\, \text{member-minimal}(m, x).
\]

\iffalse  %USE FOR WELL-FOUNDED POSETS
For every non-empty set, $x$, there is a set $y \in x$
  such that $x$ and $y$ have no elements in common.  
\fi

\item[\index{Choice axiom}Choice.]  Given a set, $s$, whose members are nonempty sets no two
  of which have any element in common, then there is a set, $c$,
  consisting of exactly one element from each set in $s$.

\iffalse
\begin{tabbing}
$\exists y \, \forall z \, \forall w \,
 \biggl( ($\=$z \in w \,\QAND\, w \in x) \; \QIMPLIES $\\
\> $\exists v \, \exists u \, \Bigl(\exists t \, \bigr((u \in w \, \QAND \, w \in t)$\=$\;\QAND\; (u \in t \,\QAND\, t \in y)\bigl) $\\
\> \> $\QIFF\; u = v\Bigr) \biggr)$
\end{tabbing}

\[\begin{array}{rlll}
\exists y \forall z \forall w & ( (z \in w \QAND w \in x) \QIMPLIES\\
                              &\quad \exists v \exists u (\exists t
                                           ((u \in w \QAND & w \in t)
                                                              & \QAND (u \in t \QAND t \in y))\\
                                                            &&& \QIFF u = v))
\end{array}\]
\fi

\end{description}


\subsection{Avoiding \idx{Russell's Paradox}}

These modern ZFC axioms for set theory are much simpler than the system
Russell and Whitehead first came up with to avoid paradox.  In fact, the
ZFC axioms are as simple and intuitive as Frege's original axioms, with
one technical addition: the Foundation axiom.  Foundation captures the
intuitive idea that sets must be built up from ``simpler'' sets in certain
standard ways.  And in particular, Foundation implies that no set is ever
a member of itself.  So the modern resolution of Russell's paradox goes as
follows: since $S \not \in S$ for all sets $S$, it follows that $W$,
defined above, contains every set.  This means $W$ can't be a set ---or it
would be a member of itself.

\subsection{\idx{Power sets} are \idx{strictly bigger}}

It turns out that the ideas behind \idx{Russell's Paradox}, which caused
so much trouble for the early efforts to formulate Set Theory, lead to a
correct and astonishing fact about infinite sets: they are \emph{not all
  the same size}.

In particular,
\begin{theorem}\label{powbig}
For any set, $A$, the \idx{power set}, $\power(A)$, is \idx{strictly bigger} than $A$.
\end{theorem}
\begin{proof}
  First of all, $\power(A)$ is as big as $A$: for example, the partial
  function $f:\power(A) \to A$, where $f(\set{a}) \eqdef a$ for $a \in A$
  and $f$ is only defined on one-element sets, is a surjection.

  To show that $\power(A)$ is strictly bigger than $A$, we have to show
  that if $g$ is a function from $A$ to $\power(A)$, then $g$ is not a
  surjection.  So, mimicking Russell's Paradox, define
  \[
  A_g \eqdef \set{a \in A \suchthat a \notin g(a)}.
  \]
  Now $A_g$ is a well-defined subset of $A$, which means it is a member of
  $\power(A)$.  But $A_g$ can't be in the range of $g$, because if it
  were, we would have
\[
A_g = g(a_0)
\]
for some $a_0 \in A$, so by definition of $A_g$,
\[
a \in g(a_0) \qiff a \in A_g \qiff a \notin g(a)
\]
for all $a \in A$.  Now letting $a = a_0$ yields the contradiction
\[
a_0 \in g(a_0) \qiff a_0 \notin g(a_0).
\]
So $g$ is not a surjection, because there is an element in the power set
of $A$, namely the set $A_g$, that is not in the range of $g$.
\end{proof}

\subsubsection{Larger Infinities}

There are lots of different sizes of infinite sets.  For example, starting
with the infinite set, $\naturals$, of nonnegative integers, we can build
the infinite sequence of sets
\[
\naturals,\ \power(\naturals),\ \power(\power(\naturals)),\
\power(\power(\power(\naturals))),\ \dots.
\]
By Theorem~\ref{powbig}, each of these sets is strictly bigger than all
the preceding ones.  But that's not all: the union of all the sets in the
sequence is strictly bigger than each set in the sequence
(see Problem~\ref{CP_power_set-tower}).  In this way you can keep going,
building still bigger infinities.

So there is an endless variety of different size infinities.

\subsection{Does All This Really Work?}\label{setsreallywork}

So this is where mainstream mathematics stands today: there is a handful
of ZFC axioms from which virtually everything else in mathematics can be
logically derived.  This sounds like a rosy situation, but there are
several dark clouds, suggesting that the essence of truth in mathematics
is not completely resolved.

%
\begin{itemize}

\item The \idx{ZFC axioms} weren't etched in stone by God.  Instead, they were
  mostly made up by some guy named Zermelo.  Probably some days he forgot
  his house keys.

  So maybe \idx{Zermelo}, just like \idx{Frege}, didn't get his axioms right and will
  be shot down by some successor to \idx{Russell} who will use his axioms to
  prove a proposition $P$ and its negation $\QNOT P$.  Then Math would be
  broken.  This sounds crazy, but after all, it has happened before.

  In fact, while there is broad agreement that the ZFC axioms are capable
  of proving all of standard Mathematics, the axioms have some further
  consequences that sound paradoxical.  For example, the \idx{Banach-Tarski}
  Theorem says that, as a consequence of the \idx{Axiom of Choice}, a solid ball
  can be divided into six pieces and then the pieces can be rigidly
  rearranged to give \emph{two} solid balls, each the same size as the
  original!

\item Georg \term{Cantor} was a contemporary of \idx{Frege} and
  \idx{Russell} who first developed the theory of infinite sizes (because
  he thought he needed it in his study of Fourier series).  Cantor raised
  the question whether there is a set whose size is strictly between the
  ``smallest\footnote{See Problem~\ref{CP_smallest_infinite_set}}''
  infinite set, $\naturals$, and $\power(\naturals)$; he guessed not:

  \textbf{Cantor's \term{Continuum Hypothesis}}: There is no set, $A$,
  such that $\power(\naturals)$ is \idx{strictly bigger} than $A$ and $A$
  is strictly bigger than $\naturals$.

  The Continuum Hypothesis remains an open problem a century later.  Its
  difficulty arises from one of the deepest results in modern Set Theory
  ---discovered in part by \idx{G\"odel} in the 1930's and Paul
  \idx{Cohen} in the 1960's ---namely, the ZFC axioms are not sufficient
  to settle the Continuum Hypothesis: there are two collections of sets,
  each obeying the laws of \idx{ZFC}, and in one collection the Continuum
  Hypothesis is true, and in the other it is false.  So settling the
  Continuum Hypothesis requires a new understanding of what Sets should be
  to arrive at persuasive new axioms that extend ZFC and are strong enough
  to determine the truth of the Continuum Hypothesis one way or the other.

\item But even if we use more or different axioms about sets, there are
  some unavoidable problems.  In the 1930's, \idx{G\"odel} proved that,
  assuming that an axiom system like \idx{ZFC} is consistent ---meaning
  you can't prove both $P$ and $\QNOT P$ for any proposition, $P$ ---then
  the very proposition that the system is consistent (which is not too
  hard to express as a logical formula) cannot be proved in the system.
  In other words, no \idx{consistent} system is strong enough to verify
  itself.
  
\end{itemize}

\subsection{Large Infinities in Computer Science}

If the romance of different size infinities and continuum hypotheses
doesn't appeal to you, not knowing about them is not going to lower your
professional abilities as a Computer Scientist.  These abstract issues
about infinite sets rarely come up in mainstream Mathematics, and they
don't come up at all in Computer Science, where the focus is generally on
``\idx{countable},'' and often just finite, sets.  In practice, only
Logicians and Set Theorists have to worry about collections that are too
big to be sets.  In fact, at the end of the 19th century, the general
Mathematical community doubted the relevance of what they called
``\idx{Cantor's paradise}'' of unfamiliar sets of arbitrary infinite size.

But the proof that power sets are bigger gives the simplest form of what
is known as a ``\idx{diagonal argument}.''  Diagonal arguments are used to
prove many fundamental results about the limitations of computation, such
as the undecidability of the \idx{Halting Problem} for programs (see
Problem~\ref{CP_recognizable_sets}) and the inherent, unavoidable,
\idx{inefficiency} (\idx{exponential time} or worse) of procedures for
other computational problems.  So Computer Scientists do need to study
diagonal arguments in order to understand the logical limits of
computation.

\begin{problems}
%\practiceproblems
%\pinput{}
\classproblems
\pinput{CP_power_set-tower}
\pinput{CP_recognizable_sets}
\pinput{CP_undescribable_language}
%\homeworkproblems
%\pinput{PS_Russells_and_cardinality}  ---subsumed by text
\end{problems}

\newpage
\section{Glossary of Symbols}
\begin{center}
\begin{tabular}{ll}
symbol &  meaning\\
\hline
$\eqdef$ & is defined to be\\
$\land$ & and\\
$\lor$ & or\\
$\implies$ & implies\\
$\neg$    & not\\
$\neg{P}$ & not $P$\\
$\bar{P}$ & not $P$\\
$\iff$    & iff\\
$\iff$    & equivalent\\
$\oplus$   & xor\\
$\exists$ & exists\\
$\forall$ & for all\\
$\in$   &  is a member of\\

\iffalse

$\subseteq$ & is a subset of\\
$\subset$ & is a proper subset of\\
$\union$  & set union\\
$\intersect$ & set intersection\\
$\bar{A}$ & complement of a set, $A$\\
$\power(A)$ & powerset of a set, $A$\\
$\emptyset$ & the empty set, $\set{}$\\
$\naturals$ & nonnegative integers \\
$\integers$ & integers\\
$\integers^+$ & positive integers\\
$\integers^-$ & negative integers\\
$\rationals$ & rational numbers\\
$\reals$ & real numbers\\
$\complexes$ & complex numbers\\
$\emptystring$ & the empty string/list
\fi

\end{tabular}
\end{center}

\endinput
