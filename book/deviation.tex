\chapter{Deviations}\label{chap:deviations}

In some cases, a random variable is likely to be very close to its
expected value.  For example, if we flip 100~fair,
mutually-independent coins, it is very likely that we will get about
50~heads.  In fact, we proved in
Section~\ref{binomial_distribution_section} that the probability of
getting fewer than 25 or more than 75~heads are each less than~$3
\cdot 10^{-7}$.  In such cases, the mean provides a lot of information
about the random variable.

In other cases, a random variable is likely to be \emph{far} from its
expected value.  For example, suppose we flipped 100~fair coins that
are glued together so that they all come out ``heads'' or they call
all come out ``tails.''  In this case, the expected value of the
number of heads is still~50, but the actual number of heads is
guaranteed to be far from this value---it will be 0 or~100, each with
probability~$1/2$.

Mathematicians have developed a variety of measures and methods to help
us understand how a random variable performs in comparison to its
mean.  The simplest and most widely used measure is called the
\term{variance} of the random variable.  The variance is a single
value associated with the random variable that is large for random
variables that are likely to deviate significantly from the mean and
that is small otherwise.

\section{Variance}\label{sec:variance}

\subsection{Definition and Examples}

Consider the following two gambling games:
\begin{description}

\item[Game A:]

You win~\$2 with probability~$2/3$ and lose~\$1 with
probability~$1/3$.

\item[Game B:]

You win~\$1002 with probability~$2/3$ and lose~\$2001 with
probability~$1/3$.

\end{description}
Which game would you rather play?  Which game is better financially?
We have the same probability, 2/3, of winning each game, but that does
not tell the whole story.  What about the expected return for each
game?  Let random variables $A$ and $B$ be the payoffs for the two
games.  For example, $A$ is 2 with probability 2/3 and -1 with
probability 1/3.  We can compute the expected payoff for each game as
follows:
\begin{align*}
    \expect{A} &= 2 \cdot \frac{2}{3} + (-1) \cdot \frac{1}{3} = 1, \\
    \expect{B} &= 1002 \cdot \frac{2}{3} + (-2001) \cdot \frac{1}{3} = 1.
\end{align*}

The expected payoff is the same for both games, but they are obviously
very different! The stakes are a lot higher for Game~B and so it is
likely to deviate much farther from its mean than is Game~A\@.  This
fact is captured by the notion of \term{variance}.

\begin{definition}\label{defvar}
The \term{variance}~$\variance{R}$ of a random variable~$R$ is
\[
    \variance{R} \eqdef \expect{(R - \expect{R})^2}.
\]
\end{definition}

In words, the variance of a random variable~$R$ is the expectation of
the square of the amount by which $R$~differs from its expectation.

Yikes!  That's a mouthful.  Try saying that 10~times in a row!

Let's look at this definition more carefully.  We'll start with~$R -
\expect{R}$.  That's the amount by which $R$~differs from its
expectation and it is obviously an important measure.  Next, we square
this value.  More on why we do that in a moment.  Finally, we take the
the expected value of the square.  If the square is likely to be
large, then the variance will be large.  If it is likely to be small,
then the variance will be small.  That's just the kind of statistic we
are looking for.  Let's see how it works out for our two gambling
games.

We'll start with Game~A:
\begin{align}
A - \expect{A}
        & = \begin{cases}
                 1 & \text{ with probability $\frac{2}{3}$} \\
                -2 & \text{ with probability $\frac{1}{3}$}
            \end{cases}
    \notag\\
(A - \expect{A})^2
        & = \begin{cases}
                1\phantom{-} & \text{ with probability $\frac{2}{3}$} \\
                4 & \text{ with probability $\frac{1}{3}$}
            \end{cases}
    \notag\\
\expect{(A - \expect{A})^2}
        & =     1 \cdot \frac{2}{3} + 4 \cdot \frac{1}{3} \notag\\
\variance{A} & =  2. \label{eqn:18A7}
\end{align}

For Game~B, we have
\begin{align*}
B - \expect{B}
        & = \begin{cases}
                1001\phantom{,001} & \text{ with probability $\frac{2}{3}$} \\
               -2002 & \text{ with probability $\frac{1}{3}$}
            \end{cases} \\
(B - \expect{B})^2
        & = \begin{cases}
                1{,}002{,}001 & \text{ with probability $\frac{2}{3}$} \\
                4{,}008{,}004 & \text{ with probability $\frac{1}{3}$}
            \end{cases} \\
\expect{(B - \expect{B})^2}
        & =     1{,}002{,}001 \cdot \frac{2}{3} + 4{,}008{,}004 \cdot \frac{1}{3} \\
\variance{B} & =   2{,}004{,}002.
\end{align*}

The variance of Game~A is~2 and the variance of Game~B is more than
two million!  Intuitively, this means that the payoff in Game~A is
usually close to the expected value of~\$1, but the payoff in Game B
can deviate very far from this expected value.

High variance is often associated with high risk.  For example, in ten
rounds of Game~A, we expect to make~\$10, but could conceivably
lose~\$10 instead.  On the other hand, in ten rounds of Game~B, we
also expect to make~\$10, but could actually lose more than~\$20,000!

\subsubsection{Why Bother Squaring?}

The variance is the average \emph{of the square} of the deviation from the
mean.  For this reason, variance is sometimes called the ``mean squared
deviation.''  But why bother squaring?  Why not simply compute the average
deviation from the mean?  That is, why not define variance to be
$\expect{R - \expect{R}}$?

The problem with this definition is that the positive and negative
deviations from the mean exactly cancel.  By linearity of expectation,
we have:
\[
  \expect{R - \expect{R}} = \expect{R} - \expect{\expect{R}}.
\]
Since $\expect{R}$ is a constant, its expected value is itself. Therefore
\[
\expect{R - \expect{R}} = \expect{R} - \expect{R} = 0.
\]
By this definition, every random variable would have zero variance,
which would not be very useful!  Because of the square in the
conventional definition, both positive and negative deviations from
the mean increase the variance, and they do not cancel.

Of course, we could also prevent positive and negative deviations from
canceling by taking an absolute value.  In other words, we could
compute $\expect{\ \abs{R - \expect{R}}\ }$.  But this measure doesn't
have the many useful properties that variance has, and so
mathematicians went with squaring.

\subsection{Standard Deviation}

Because of its definition in terms of the square of a random variable,
the variance of a random variable may be very far from a typical
deviation from the mean.  For example, in Game B above, the deviation
from the mean is 1001 in one outcome and -2002 in the other. But the
variance is a whopping 2,004,002.

From a dimensional analysis viewpoint, the ``units'' of variance are
wrong: if the random variable is in dollars, then the expectation is
also in dollars, but the variance is in square dollars.  

For these reasons, people often describe the deviation of a random
variable using \term{standard deviation} instead of variance.

\begin{definition}
The \term{standard deviation}~$\sigma_R$ of a random variable~$R$~is
the square root of the variance:
\[
    \sigma_R \eqdef \sqrt{\variance{R}} = \sqrt{\expect{(R - \expect{R})^2}}.
\]
\end{definition}

So the standard deviation is the square root of the mean of the square
of the deviation, or the \term{root mean square} for short.  It has
the same units---dollars in our example---as the original random
variable and as the mean.  Intuitively, it measures the average
deviation from the mean, since we can think of the square root on the
outside as roughly canceling the square on the inside.

For example, the standard deviations for $A$ and~$B$ are
\begin{align*}
    \sigma_A & = \sqrt{\variance{A}} = \sqrt{2} \approx 1.41, \\
    \sigma_B & = \sqrt{\variance{B}} = \sqrt{2,004,002} \approx 1416.
\end{align*}
The random variable~$B$ actually deviates from the mean by either
positive 1001 or negative 2002; therefore, the standard deviation of
1416 describes this situation reasonably well.

\subsection{An Alternative Formulation}

Applying linearity of expectation to the formula for variance yields a
convenient alternative formula.
\begin{lemma}\label{alt:var}
For any random variable~$R$,
\[
    \variance{R} = \expect{R^2} - \expectsq{R}.
\]
\end{lemma}
Here we use the notation \idx{$\expectsq{R}$} as shorthand for
$(\expect{R})^2$.  Remember that $\expect{R^2}$ is generally not equal
to $\expectsq{R}$.  We know the expected value of a product is the
product of the expected values for independent variables, but not in
general.  And $R$ is not independent of itself unless it is constant.

\begin{proof}[Proof of Lemma~\ref{alt:var}]
Let $\mu = \expect{R}$.  Then
\begin{align*}
\variance{R} & =   \expect{(R - \expect{R})^2}
               & \text{(Definition~\ref{defvar} of variance)}\\
        & = \expect{(R - \mu)^2} & \text{(definition of $\mu$)}\\
        & = \expect{R^2 - 2  \mu R + \mu^2} \\
        & = \expect{R^2} - 2 \mu \expect{R} + \mu^2
                & \text{(linearity of expectation)}\\
        & = \expect{R^2} - 2 \mu^2 + \mu^2
              &  \text{(definition of $\mu$)}\\
        & = \expect{R^2} - \mu^2\\
        & = \expect{R^2} - \expectsq{R}.
                  &  \text{(definition of $\mu$)} & \qedhere
\end{align*}
\end{proof}

For example, let's take another look at Game~A from
Section~\ref{sec:variance} where you win~\$2 with probability~$2/3$
and lose~\$1 with probability~$1/3$.  Then
\begin{equation*}
    \expect{A} = 2 \cdot \frac{2}{3} + (-1) \cdot \frac{1}{3} = 1
\end{equation*}
and
\begin{equation*}
    \expect{A^2} = 4 \cdot \frac{2}{3} + 1 \cdot \frac{1}{3} = 3.
\end{equation*}
By Lemma~\ref{alt:var}, this means that
\begin{equation*}
\variance{A}
    = \expect{A^2} - \expectsq{A}
    = 3 - 1^2 
    = 2,
\end{equation*}
confirming the result in Equation~\ref{eqn:18A7}.

The alternate formulation of variance given in Lemma~\ref{alt:var} has
a cute implication:
\begin{corollary}
If $R$ is a random variable, then $\expect{R^2} \geq \expectsq{R}$.
\end{corollary}
\begin{proof}
We defined $\variance{R}$ as an average of a squared expression, so
$\variance{R}$ is nonnegative.  Then we proved that $\variance{R} =
\expect{R^2} - \expectsq{R}$.  This implies that $\expect{R^2} -
\expectsq{R}$ is nonnegative.  Therefore, $\expect{R^2} \geq
\expectsq{R}$.
\end{proof}

In words, the expectation of a square is at least the square of the
expectation. The two are equal exactly when the variance is zero:
\begin{equation*}
\expect{R^2} = \expectsq{R}
    \quad \text{iff}\quad \expect{R^2} - \expectsq{R} = 0
    \quad \text{iff}\quad \variance{R} = 0.
\end{equation*}
This happens precisely when
\begin{equation*}
    \Prob{R = \expect{R}} = 1;
\end{equation*}
namely, when $R$~is a constant.\footnote{Technically, $R$~could deviate
  from its mean on some sample points with probability~0, but we are
  ignoring events of probability~0 when computing expectations and
  variances.}

\subsection{Indicator Random Variables}

Computing the variance of an indicator random variable is
straightforward given Lemma~\ref{alt:var}.

\begin{lemma}\label{bernoulli-variance}
Let $B$~be an indicator random variable for which $\prob{B = 1} = p$.
Then
\begin{equation}
    \variance{B} = p-p^2 = p(1-p).
\end{equation}
\end{lemma}

\begin{proof}
  By Lemma~\ref{expindic}, $\expect{B}= p$.  But since $B$ only takes
  values 0 and 1, $B^2 = B$.  So 
\begin{equation*}
    \variance{B} = \expect{B^2} - \expectsq{B} = p - p^2,
\end{equation*}
as claimed.
\end{proof}

For example, let $R$~be the number of heads when you flip a single
fair coin.  Then
\begin{equation}\label{eqn:18H1}
    \variance{R} = \frac{1}{2} - \paren{\frac{1}{2}}^2 = \frac{1}{4}
\end{equation}
and
\begin{equation*}
    \sigma_R = \sqrt{\frac{1}{4}} = \frac{1}{2}.
\end{equation*}

\subsection{Mean Time to Failure}

As another example, consider the mean time to failure problem,
described in Section~\ref{mean_time_to_failure_subsec}.  If the system
crashes at each step with probability~$p$, then we already know that
the mean time to failure is~$1/p$.  In other words, if $C$~is the
number of steps up to and including the step when the first crash
occurs, then
\begin{equation*}
    \expect{C} = \frac{1}{p}.
\end{equation*}

What about the variance of~$C$?  To use Lemma~\ref{alt:var}, we need
to compute~$\expect{C^2}$.  As in
Section~\ref{mean_time_to_failure_subsec}, we can do this by summing
over all the sample points or we can use the Law of Total
Expectation.  The latter approach is simpler, so we'll do that.  The
analysis breaks into two cases: the system crashes in the first step
or it doesn't.  Hence,
\begin{align*}
\expect{C^2}
    &= 1^2 \cdot p + \expect{(C + 1)^2} (1 - p) \\
    &= p + \expect{C^2} (1 - p) + 2 \expect{C} (1 - p) + (1 - p) \\
    &= 1 + \expect{C^2} (1 - p) + 2 \paren{\frac{1 - p}{p}}.
\end{align*}
Simplifying, we find that
\begin{equation*}
    p \expect{C^2} = \frac{2 - p}{p}
\end{equation*}
and that
\begin{equation*}
    \expect{C^2} = \frac{2 - p}{p^2}.
\end{equation*}
Using Lemma~\ref{alt:var}, we conclude that
\begin{align*}
\variance{C}
    &= \expect{C^2} - \expectsq{C} \\
    &= \frac{2 - p}{p^2} - \frac{1}{p^2} \\
    &= \frac{1 - p}{p^2}.
\end{align*}

\subsection{Uniform Random Variables}

Computing the variance of a uniform random variable is also
straightforward given Lemma~\ref{alt:var}.  For example, we can
compute the variance of the outcome of a fair die~$R$ as follows:
\begin{align*}
\expect{R^2}
    &= \frac{1}{6} (1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2) = \frac{91}{6},\\
\expectsq{R}
    &= \paren{3\frac{1}{2}}^2 = \frac{49}{4}, \\
\variance{R}
    &= \expect{R^2} - \expectsq{R} = \frac{91}{6} - \frac{49}{4}
        = \frac{35}{12}.
\end{align*}

For a general uniform random variable~$R$ on~$\set{1, 2, 3, \dots n}$,
the variance can be computed as follows:
\begingroup
\openup\jot
\begin{align*}
\expect{R}
    &= \frac{1}{n} (1 + 2 + \dots + n) \\
    &= \frac{1}{n} \cdot \frac{n (n + 1)}{2} \\
    &= \frac{n + 1}{2}. \\[\jot]
%
\expect{R^2}
    &= \frac{1}{n} (1^2 + 2^2 + \dots + n^2) \\
    &= \frac{1}{n} \cdot \frac{(2n + 1) n (n + 1)}{6} \\
    &= \frac{(2n + 1) (n + 1)}{6}. \\[\jot]
%
\variance{R}
    &= \expect{R^2} - \expectsq{R} \\
    &= \frac{(2n + 1) (n + 1)}{6} - \paren{\frac{n + 1}{2}}^2 \\
%    &= \frac{4n^2 + 2n - 3n^2}{12} \\
    &= \frac{n^2 - 1}{12}.
\end{align*}
\endgroup

\subsection{Dealing with Constants}

It helps to know how to calculate the variance of $a R + b$:

\begin{theorem}\label{var.const}\label{var+const}\label{thm:var(aR+b)}
Let $R$ be a random variable, and let $a$ and~$b$ be constants. Then
\begin{equation}\label{a2R}
    \variance{a R + b} = a^2 \variance{R}.
\end{equation}
\end{theorem}

\begin{proof}
Beginning with Lemma~\ref{alt:var} and repeatedly applying linearity
of expectation, we have:
\begin{align*}
\variance{aR}
    &= \expect{(a R + b)^2} - \expectsq{a R + b} \\
    &= \expect{a^2 R^2 + 2 a b R + b^2}
            - \paren{a \expect{R} + b}^2 \\
    &= a^2 \expect{R^2} + 2 a b \expect{R} + b^2
        - a^2 \expectsq{R} - 2 a b \expect{R} - b^2 \\
    & = a^2\expect{R^2} -a^2\expectsq{R}\\
    & = a^2\paren{\expect{R^2} - \expectsq{R}}\\
    & = a^2\variance{R} \qquad\qquad \text{(by Lemma~\ref{alt:var})}. & \qedhere
\end{align*}
\end{proof}

\begin{corollary}
\[
\sigma_{aR+b} = \abs{a}\sigma_{R}.
\]
\end{corollary}

\subsection{Variance of a Sum}

In general, the variance of a sum is not equal to the sum of the
variances, but variances do add for \emph{\idx{independent}} random
variables.  In fact, \index{mutual independence} \emph{mutual}
independence is not necessary: \index{pairwise independence}
\emph{pairwise} independence will do.

\begin{theorem}\label{indvar}
If $R_1$ and $R_2$ are independent random variables, then
\begin{equation}\label{vR+R}
\variance{R_1 + R_2} = \variance{R_1} + \variance{R_2}.
\end{equation}
\end{theorem}

\begin{proof}

As with the proof of Theorem~\ref{thm:var(aR+b)}, this proof uses
repeated applications of Lemma~\ref{alt:var} and Linearity of
Expectation.
\begin{align*}
\variance{R_1 + R_2}
    &= \expect{(R_1 + R_2)^2} - \expectsq{R_1 + R_2} \\
    &= \expect{R_1^2 + 2 R_1 R_2 + R^2}
            - \paren{\expect{R_1} + \expect{R_2}}^2 \\
    &= \expect{R_1^2} + 2 \expect{R_1 R_2} + \expect{R_2^2} \\
    &\qquad\qquad
            - \expectsq{R_1} - 2 \expect{R_1} \expect{R_2}
            - \expectsq{R_2} \\
    &= \variance{R_1} + \variance{R_2}
            + 2( \expect{R_1 R_2} - \expect{R_1} \expect{R_2} ) \\
    &= \variance{R_1} + \variance{R_2}.
\end{align*}
The last step follows because
\begin{equation*}
    \expect{R_1 R_2} = \expect{R_1} \expect{R_2}
\end{equation*}
when $R_1$ and~$R_2$ are independent.
\end{proof}

Note that Theorem~\ref{indvar} does not necessarily hold if $R_1$
and~$R_2$ are dependent since then it would generally not be true that
\begin{equation}\label{eqn:18J1}
    \expect{R_1 R_2} = \expect{R_1} \expect{R_2}
\end{equation}
in the last step of the proof.  For example, suppose that $R_1 = R_2 =
R$.  Then Equation~\ref{eqn:18J1} holds only if $R$~is essentially
constant.

The proof of Theorem~\ref{indvar} carries over straightforwardly to
the sum of any finite number of variables.

\begin{theorem}[Pairwise Independent Additivity of Variance]
\label{thm:variance_additivity}
If $R_1$, $R_2$, \dots, $R_n$ are \emph{pairwise} independent random
variables, then
\begin{equation}\label{vsum}
\variance{R_1 + R_2 + \cdots + R_n} = \variance{R_1} + \variance{R_2} +
  \cdots + \variance{R_n}.
\end{equation}
\end{theorem}

Unfortunately, there is no product rule for computing variances, even
if the random variables are mutually independent.  However, we can use
Theorem~\ref{thm:variance_additivity} to quickly compute the variance
of a random variable with a general binomial distribution.

\subsection{Binomial Distributions}

\begin{lemma}[Variance of the Binomial Distribution]
\label{lem:binomial_variance}
If $J$ has a binomial distribution with parameters $n$ and~$p$, then
\begin{equation}\label{p1p}
\variance{J} = n p (1-p).
\end{equation}
\end{lemma}

\begin{proof}

From the definition of the binomial distribution, we can think of
$J$~as being the number of ``heads'' when you flip $n$~mutually
independent coins, each of which is ``heads'' with probability~$p$.
Thus $J$~can be expressed as the sum of $n$~mutually independent
indicator variables~$J_i$ where
\begin{equation*}
    \prob{J_i = 1} = p
\end{equation*}
for~$1 \le i \le n$.  From Lemma~\ref{bernoulli-variance}, we know
that
\begin{equation*}
    \variance{J_i} = p (1 - p).
\end{equation*}
By Theorem~\ref{thm:variance_additivity}, this means that
\begin{equation*}
\variance{J}
    = \sum_{i = 1}^n \variance{J_i}
    = n p (1 - p),
\end{equation*}
as claimed.
\end{proof}

For example, suppose we flip $n$~mutually
independent\footnote{Actually, we only need to assume pairwise
  independence for this to be true using
  Theorem~\ref{thm:variance_additivity}.} fair coins.  Let $R$~be the
number of heads.  Then Theorem~\ref{thm:variance_additivity} tells us
that
\begin{equation*}
    \variance{R} = n \paren{\frac{1}{2}} \paren{1 - \frac{1}{2}}
                 = \frac{n}{4}.
\end{equation*}
Hence,
\begin{equation*}
    \sigma_R = \frac{\sqrt{n}}{2}.
\end{equation*}
This value is small compared with
\begin{equation*}
    \expect{R} = \frac{n}{2},
\end{equation*}
which should not be surprising since we already knew from
Section~\ref{binomial_distribution_section} that $R$~is unlikely to
stray very far from its mean.

\section{Markov's Theorem}

The variance of a random variable gives us a rough idea of the amount
by which a random variable is likely to deviate from its mean.  But it
does not directly give us specific bounds on the probability that the
deviation exceeds a specified threshold.  To obtain such specific
bounds, we'll need to work a little harder.

In this section, we derive a famous result known as Markov's Theorem
that gives an upper bound on the probability that a random variable
exceeds a specified threshold.  In the next section, we give a similar
but stronger result known as Chebyshev's Theorem.  The difference
between these results is that Markov's Theorem depends only on the
mean of the random variable, whereas Chebyshev's Theorem makes use of
the mean \emph{and} the variance.  Basically, the more you know about
a random variable, the better bounds you can derive on the probability
that it deviates from its mean.

\subsection{A Motivating Example}


The idea behind \idx{Markov's Theorem} can be explained with a simple
example involving \emph{intelligence quotients}, or \IQ s.  This
quantity was devised so that the average \IQ\ measurement would
be~100.  From this fact alone we can conclude that at most 1/3 the
population can have an \IQ\ of 300 or more, because if more than a
third had an \IQ\ of at least~300, then the average~\IQ\ would have to
be \emph{more} than $(1/3)300 = 100$, contradicting the fact that the
average is 100.  So the probability that a randomly chosen person has
an \IQ\ of 300 or more is at most 1/3.  Of course this is not a very
strong conclusion since no \IQ\ over~200 has ever been recorded.

By the same logic, we can also conclude that at most 2/3 of the
population can have an \IQ\ of 150 or more.  \IQ's over 150 have
certainly been recorded, although a much smaller fraction than 2/3 of
the population actually has an \IQ\ that high.

Although these conclusions about \IQ\ are weak, they are actually the
strongest general conclusions that can be reached about a random
variable using \emph{only} the fact that it is nonnegative and its
mean is 100.  For example, if we choose a random variable equal to 300
with probability 1/3, and 0 with probability 2/3, then its mean is
100, and the probability of a value of 300 or more really is 1/3.  So
we can't hope to get a better upper bound based solely on this limited
amount of information.

Markov's Theorem characterizes the bounds that can be achieved with
this kind of analysis

\subsection{The Theorem}

\begin{theorem}[\idx{Markov's Theorem}]\label{thm:markov}
  If $R$~is a nonnegative random variable, then for all~$x > 0$,
  \begin{equation*}
    \prob{R \geq x} \leq \frac{\expect{R}}{x}.
  \end{equation*}
\end{theorem}

\begin{proof}%[Proof of Markov's Theorem]
For any $x > 0$
\begin{align}
  \expect{R}
  & = \sum_{y \in \range{R}} y\prob{R=y}\notag\\
  & \geq \sum_{\substack{y \geq x,\\ y \in \range{R}}} y\prob{R=y} & \text{(because $R \geq 0$)}\notag\\
  & \geq \sum_{\substack{y \geq x,\\ y \in \range{R}}} x\prob{R=y}\notag\\
  & = x \sum_{\substack{y \geq x,\\ y \in \range{R}}} \prob{R=y}\notag\\
  & = x \prob{R \geq x}.\label{markovproof}
\end{align}
Hence,
\begin{equation*}
    \prob{R \ge x} \le \frac{\expect{R}}{x},
\end{equation*}
as claimed.
\end{proof}

\begin{corollary}
If $R$~is a nonnegative random variable, then for all~$c \geq 1$,
\begin{equation}
\Prob{R \geq c \cdot \expect{R}}
    \leq \frac{1}{c}.\label{markovaboveemean}
\end{equation}
\end{corollary}

\begin{proof}
    Set $x = c \expect{R}$ in Theorem~\ref{thm:markov}.
\end{proof}

As an example, suppose we flip 100~fair coins and use Markov's Theorem
to compute the probability of getting all heads:
\[
\prob{\text{heads} \geq 100} \leq \frac{\expect{\text{heads}}}{100} =
\frac{50}{100} = \frac{1}{2}.
\]
If the coins are mutually independent, then the actual probability of
getting all heads is a minuscule 1 in $2^{100}$.  In this case, Markov's
Theorem looks very weak.  However, in applying Markov's Theorem, we made
no independence assumptions.  In fact, if all the coins are glued
together, then probability of throwing all heads is exactly $1/2$.
In this nasty case, Markov's Theorem is actually tight!

\subsubsection{The Chinese Appetizer Problem}

Suppose that $n$~people are seated at a circular table and that each
person has an appetizer in front of them on a rotating Chinese banquet
tray.  Just as everyone is about to dig in, some joker spins the tray
so that each person receives a random appetizer.  We are interested in
the number of people~$R$ that get their same appetizer as before,
assuming that the $n$~appetizers are all different.

Each person gets their original appetizer with probability~$1/n$.
Hence, by Linearity of Expectation,
\begin{equation*}
    \expect{R} = n \cdot \frac{1}{n} = 1.
\end{equation*}
What is the probability that all $n$~people get their original
appetizer back?  Markov's Theorem tells us that
\begin{equation*}
\prob{R = n}
    = \prob{R \ge n} 
    \le \frac{\expect{R}}{n}
    = \frac{1}{n}.
\end{equation*}
In fact, this bound is tight sine everyone gets their original
appetizers back if and only if the rotating tray returns to its
original configuration, which happens with probability~$1/n$.

The Chinese Appetizer problem is similar to the Hat Check problem that
we studied in Section~\ref{sec:hat_check}, except that no distribution
was specified in the Hat Check problem---we were told only that each
person gets their correct hat back with probability~$1/n$.  If the
hats are scrambled according to uniformly random permutations, then
the probability that everyone gets the right hat back is~$1/n!$, which
is much less than the $1/n$~upper bound given by Markov's Theorem.
So, in this case, the bound given by Markov's Theorem is not close to
the actual probability.

What is the probability that at least two people get their right hats
back?  Markov's Theorem tells us that
\begin{equation*}
    \prob{R \ge 2} \le \frac{\expect{R}}{2} = \frac{1}{2}.
\end{equation*}
In this case, Markov's Theorem is not too far off from the right
answer if the hats are distributed according to a random
permutation\footnote{Proving this requires some effort.} but it is not
very close to the correct answer~$1/n$ for the case when the hats are
distributed as in the Chinese Appetizer problem.

\subsubsection{Why $R$ Must be Nonnegative}

Remember that Markov's Theorem applies only to nonnegative random
variables!  Indeed, the theorem is false if this restriction is
removed.  For example, let $R$~be -10 with probability~$1/2$ and 10
with probability~$1/2$.  Then
\[
\expect{R} = -10 \cdot \frac{1}{2} + 10 \cdot \frac{1}{2} = 0.
\]
Suppose that we now tried to compute $\prob{R \geq 5}$ using Markov's
Theorem:
\begin{equation*}
  \prob{R \geq 5} \leq \frac{\expect{R}}{5} = \frac{0}{5} = 0.
\end{equation*}
This is the wrong answer!  Obviously, $R$ is at least 5 with
probability $1/2$.

On the other hand, we can still apply Markov's Theorem indirectly to
derive a bound on the probability that an arbitrary variable like $R$
is 5 or~more.  For example, given any random variable, $R$ with expectation 0
and values $\geq -10$, we can conclude that $\prob{R \geq 5} \le 2/3$.
To prove this fact, we define $T \eqdef R+10$.  Then $T$~is a
nonnegative random variable with expectation $\expect{R + 10} =
\expect{R}+10= 10$, so Markov's Theorem applies and tells us that
$\prob{T \geq 15} \le 10/15 = 2/3$.  But $T \geq 15$ iff $R \geq 5$, so
$\prob{R \geq 5} \leq 2/3$, as claimed.

\subsection{Markov's Theorem for Bounded Variables}

Suppose we learn that the average \IQ\ among MIT students is 150
(which is not true, by the way).  What can we say about the
probability that an MIT student has an \IQ\ of more than 200?
Markov's Theorem immediately tells us that no more than $150/200$ or
$3/4$ of the students can have such a high \IQ.  That's because if
$R$~is the \IQ\ of a random MIT student, then
\[
    \prob{R > 200} \leq \frac{\expect{R}}{200}= \frac{150}{200} = \frac{3}{4}.
\]

But let's also suppose that no MIT student has an \IQ\ less than~100
(which may be true).  This means that if we let $T \eqdef R-100$, then
$T$ is nonnegative and $\expect{T} = 50$, so we can apply Markov's
Theorem to $T$ and conclude:
\[
\prob{R > 200} = \prob{T > 100} \leq \frac{\expect{T}}{100}= \frac{50}{100} =
\frac{1}{2}.
\]
So only half, not 3/4, of the students can be as amazing as they think
they are.  A bit of a relief!

More generally, we can get better bounds applying Markov's Theorem to
$R-l$ instead of $R$ for any lower bound~$l$ on~$R$, even when $l$~is
negative.

\begin{theorem}\label{thm:18C1}
Let $R$~be a random variable for which $R \ge l$ for some $l \in
\reals$.  Then for all~$x \ge l$,
\begin{equation*}
    \prob{R \ge x} \le \frac{\expect{R} - l}{x - l}.
\end{equation*}
\end{theorem}

\begin{proof}
Define
\begin{equation*}
    T \eqdef R - l.
\end{equation*}
Then $T$~is a nonnegative random variable with mean
\begin{equation*}
\expect{T} = \expect{R - l} = \expect{R} - l.
\end{equation*}
Hence, Markov's Theorem implies that
\begin{align*}
\prob{T \ge x - l}
    &\le \frac{\expect{T}}{x - l} \\
    &=   \frac{\expect{R} - l}{x - l}.
\end{align*}
The result then follows from the fact that
\begin{align*}
\prob{R \ge x}
    &= \prob{R - l \ge x - l} \\
    &= \prob{T \ge x - l}. \qedhere
\end{align*}
\end{proof}

\subsection{Deviations Below the Mean}

Markov's Theorem says that a random variable is unlikely to greatly
exceed the mean.  Correspondingly, there is a variation of Markov's
Theorem that says a random variable is unlikely to be much smaller
than its mean.

\begin{theorem}
\label{th:below}
Let $u \in \reals$ and let $R$~be a random variable such that $R \leq
u$.  Then for all~$x < u$, 
\[
    \prob{R \leq x} \leq \frac{u - \expect{R}}{u - x}.
\]
\end{theorem}

\begin{proof}
The proof is similar to that of Theorem~\ref{thm:18C1}.  Define
\begin{equation*}
    S \eqdef u - R.
\end{equation*}
Then $S$~is a nonnegative random variable with mean
\begin{equation*}
    \expect{S} = \expect{u - R} = u - \expect{R}.
\end{equation*}
Hence, Markov's Theorem implies that
\begin{equation*}
\prob{S \ge u - x}
     \le \frac{\expect{S}}{u - x}
    = \frac{u - \expect{R}}{u - x}.
\end{equation*}
The result then follows from the fact that
\begin{equation*}
\prob{R \le x}
    = \prob{u - S \le x}
    = \prob{S \ge u - x}. \qedhere
\end{equation*}
\end{proof}

For example, suppose that the class average on a midterm was
75/100.  What fraction of the class scored below 50?

There is not enough information here to answer the question exactly,
but Theorem~\ref{th:below} gives an upper bound.  Let $R$ be the score
of a random student.  Since 100 is the highest possible score, we can
set $u = 100$ to meet the condition in the theorem that $R \leq u$.
Applying Theorem~\ref{th:below}, we find:
\begin{equation*}
  \prob{R \leq 50} \leq \frac{100 - 75}{100 - 50} = \frac{1}{2}\,.
\end{equation*}

That is, at most half of the class scored 50 or worse.  This makes
sense; if more than half of the class scored 50 or worse, then the
class average could not be 75, even if everyone else scored 100.  As
with Markov's Theorem, Theorem~\ref{th:below} often gives weak
results.  In fact, based on the data given, the \emph{entire} class
could have scored \emph{above}~50.

\subsection{Using Markov's Theorem to Analyze Non-Random Events}

In the previous example, we used a theorem about a random variable to
conclude facts about non-random data.  For example, we concluded that
if the average score on a test is 75, then at most $1/2$ the class
scored 50 or worse.  There is no randomness in this problem, so how
can we apply Theorem~\ref{th:below} to reach this conclusion?

The explanation is not difficult.  For any set of scores $S =
\set{s_1, s_2, \dots, s_n}$, we introduce a random variable~$R$ such
that
\[
\prob{R = s_i} = \frac{\text{(\# of students with score $s_i$)}}{n}.
\]
We then use Theorem~\ref{th:below} to conclude that $\prob{R \leq 50}
\leq 1/2$.  To see why this means (with certainty) that at most
$1/2$ of the students scored 50 or less, we observe that
\begin{align*}
\prob{R \leq 50}
    & = \sum_{s_i \leq 50} \prob{R = s_i} \\
    & = \sum_{s_i \leq 50} \frac{\text{(\# of students with score $s_i$)}}{n} \\
    & = \frac{1}{n} \text{(\# of students with score 50 or less)}.
\end{align*}
So, if $\prob{R \leq 50} \leq 1/2$, then the number of students
with score 50 or less is at most $n/2$.

\begin{problems}

\classproblems
\pinput{CP_cold_cows_markov}

\end{problems}

\section{Chebyshev's Theorem}

As we have just seen, Markov's Theorem can be extended by applying it
to functions of a random variable~$R$ such as $R - l$ and $u - R$.
Even stronger results can be obtained by applying Markov's Theorem to
powers of~$R$.

\begin{lemma}\label{lem:Markov2}
For any random variable~$R$, $\alpha \in \reals^+$, and $x > 0$,
\[
\prob{\abs{R} \geq x} \leq \frac{\expect{\abs{R}^\alpha}}{x^\alpha}.
\]
\end{lemma}

\begin{proof}
The event $\abs{R} \ge x$ is the same as the event $\abs{R}^\alpha \ge
x^\alpha$.  Since $\abs{R}^\alpha$~is nonnegative, the result follows
immediately from Markov's Theorem.
\end{proof}

Similarly,

\begin{equation}\label{chebE2}
\prob{\abs{R - \expect{R}} \geq x}
    \leq \frac{\expect{(R - \expect{R})^\alpha}}{x^\alpha}.
\end{equation}

The restatement of~Equation \ref{chebE2} for $\alpha=2$ is known as
\term{Chebyshev's Theorem}.
\begin{theorem}[Chebyshev]\label{chebthm}
  Let $R$ be a random variable and $x \in \reals^+$.  Then
\[
\prob{\abs{R - \expect{R}} \geq x} \leq \frac{\variance{R}}{x^2}.
\]
\end{theorem}

\begin{proof}

Define
\begin{equation*}
    T \eqdef R - \expect{R}.
\end{equation*}
Then
\begin{align*}
\prob{ \abs{R - \expect{R}} \ge x }
    &= \prob{\abs{T} \ge x} \\
     &= \prob{T^2 \ge x^2} \\
     &\le \frac{\expect{T^2}}{x^2} && \text{(by Markov's Theorem)} \\
     & = \frac{ \expect{ (R - \expect{R})^2 } }{x^2} \\
     &= \frac{\variance{R}}{x^2},  && \text{(by Definition~\ref{defvar})}
\end{align*}
as claimed.
\end{proof}


\begin{corollary}
\label{cor:cheby}
Let $R$ be a random variable, and let $c$~be a positive real number.
\[
\prob{\abs{R - \expect{R}} \geq c \sigma_R} \leq \frac{1}{c^2}.
\]
\end{corollary}

\begin{proof}
  Substituting $x = c \sigma_R$ in Chebyshev's Theorem gives:
  \begin{equation*}
    \prob{\card{R - \expect{R}} \geq c \sigma_R}
    \leq
    \frac{\variance{R}}{(c \sigma_R)^2}
    =  \frac{\sigma_R^2}{(c \sigma_R)^2}
    = \frac{1}{c^2}.\qedhere
  \end{equation*}
\end{proof}

As an example, suppose that, in addition to the national average
\idx{\IQ}\ being 100, we also know the \idx{standard deviation} of
\IQ's is 10.  How rare is an \IQ\ of 300 or more?

Let the random variable~$R$ be the \IQ\ of a random person.  So we are
supposing that $\expect{R} = 100$, $\sigma_R = 10$, and $R$ is
nonnegative.  We want to compute $\prob{R \geq 300}$.

We have already seen that Markov's Theorem~\ref{thm:markov} gives a
coarse bound, namely,
\[
  \prob{R \geq 300} \leq \frac{1}{3}.
\]
Now we apply Corollary~\ref{cor:cheby} to the same problem:
\begin{equation}\label{eqn:18I3}
\prob{R \geq 300}
    \le \Prob{\abs{R - 100} \geq 20 \sigma_r}
    \le \frac{1}{400}.
\end{equation}

So Chebyshev's Theorem implies that at most one person in four hundred
has an \IQ\ of 300 or more.  We have gotten a much tighter bound using
the additional information, namely the standard deviation of~$R$, than
we could get knowing only the expectation.

More generally, Corollary~\ref{cor:cheby} tells us that a random
variable is never likely to stray by more than a few standard
deviations from its mean.  For example, plugging~$c = 3$ into
Corollary~\ref{cor:cheby}, we find that the probability that a random
variable strays from the mean by more than~$3 \sigma$ is at
most~$1/9$.

This fact has a nice pictorial characterization for pdf's with a
``bell-curve'' shape; namely, the width of the bell is~$O(\sigma)$, as
shown in Figure~\ref{fig:stdev}.

\begin{figure}

\graphic[height=2in]{stdev}

\caption{If the pdf of a random variable is ``bell-shaped,'' then the
  width of the bell is~$O(\sigma)$.}

\label{fig:stdev}

\end{figure}

\subsection{Bounds on One-Sided Errors}

Corollary~\ref{cor:cheby} gives bounds on the probability of deviating
from the mean in \emph{either} direction.  If you only care about
deviations in one direction, as was the case in the \IQ\ example, then
slightly better bounds can be obtained.

\begin{theorem}\label{thm:18I2}
For any random variable~$R$ and any~$c > 0$,
\begin{equation*}
    \prob{R - \expect{R} \ge c \sigma_R} \le \frac{1}{c^2 + 1}
\end{equation*}
and
\begin{equation*}
    \prob{R - \expect{R} \le -c \sigma_R} \le \frac{1}{c^2 + 1}.
\end{equation*}
\end{theorem}

The proof of Theorem~\ref{thm:18I2} is trickier than the proof of
Chebyshev's Theorem and we will not give the details here.  Nor will
we prove the fact that
the bounds in Theorem~\ref{thm:18I2} are the best bounds that you can
obtain if you know only the mean and standard deviation of the random
variable~$R$.

Returning to the \IQ\ example, Theorem~\ref{thm:18I2} tells us that
\begin{equation*}
    \prob{R \ge 300} \le \prob{R - 100 \ge 20 \sigma_R} \le \frac{1}{401},
\end{equation*}
which is a \emph{very slight} improvement over Equation~\ref{eqn:18I3}.

As another example, suppose we give an exam.  What fraction of the
class can score more than 2~standard deviations from the average?  If
$R$ is the score of a random student, then
\begin{equation*}
    \prob{\abs{R - \expect{R}} \ge 2 \sigma_R} \le \frac{1}{4}.
\end{equation*}
For one-sided error, the fraction that could be 2~standard deviations
or more above the average is at most
\begin{equation*}
    \frac{1}{2^2 + 1} = \frac{1}{5}.
\end{equation*}
This results holds no matter what the test scores are, and is again a
deterministic fact derived using probabilistic tools.

\section{Bounds for Sums of Random Variables}

If all you know about a random variable is its mean and variance, then
Chebyshev's Theorem is the best you can do when it comes to bounding
the probability that the random variable deviates from its mean.  In
some cases, however, we know more---for example, that the random
variable has a binomial distribution---and then it is possible to
prove much stronger bounds.  Instead of polynomially small bounds such
as~$1/c^2$, we can sometimes even obtain exponentially small bounds
such as~$1/e^c$.  As we will soon discover, this is the case whenever
the random variable~$T$ is the sum of $n$~mutually independent random
variables~$T_1$, $T_2$, \dots, $T_n$ where $0 \le T_i \le 1$.  A
random variable with a binomial distribution is just one of many
examples of such a~$T$.  Here is another.

\subsection{A Motivating Example}

Fussbook is a new social networking site oriented toward unpleasant
people.

Like all major web services, Fussbook has a \idx{load balancing} problem.
Specifically, Fussbook receives 24,000 forum posts every 10 minutes.
Each post is assigned to one of $m$ computers for processing, and each
computer works sequentially through its assigned tasks.  Processing an
average post takes a computer $1/4$ second.  Some posts, such as
pointless grammar critiques and snide witticisms, are easier.  But the
most protracted harangues require 1 full second.

Balancing the work load across the $m$ computers is vital; if any
computer is assigned more than 10 minutes of work in a 10-minute
interval, then that computer is overloaded and system performance
suffers.  That would be bad, because Fussbook users are \emph{not} a
tolerant bunch.

An early idea was to assign each computer an alphabetic range of forum
topics.  (``That oughta work!'', one programmer said.)  But after the
computer handling the ``\emph{pr}ivacy'' and ``\emph{pr}eferred text
editor'' threads melted, the drawback of an ad hoc approach was clear:
there are no guarantees.

If the length of every task were known in advance, then finding a
balanced distribution would be a kind of ``\idx{bin packing}''
problem.  Such problems are hard to solve exactly, though
approximation algorithms can come close.  But in this case, task
lengths are not known in advance, which is typical for workload
problems in the real world.

So the load balancing problem seems sort of hopeless, because there is
no data available to guide decisions.  Heck, we might as well assign
tasks to computers at random!

As it turns out, random assignment not only balances load reasonably
well, but also permits provable performance guarantees in place of
``That oughta work!''  assertions.  In general, a randomized approach
to a problem is worth considering when a deterministic solution is
hard to compute or requires unavailable information.

Some arithmetic shows that Fussbook's traffic is sufficient to keep $m
= 10$ computers running at 100\% capacity with perfect load balancing.
Surely, more than 10 servers are needed to cope with random
fluctuations in task length and imperfect load balance.  But how many
is enough?  11?  15?  20?  100? We'll answer that question with a new
mathematical tool.

\subsection{The \idx{Chernoff Bound}}

The Chernoff\footnote{Yes, this is the same Chernoff who figured out
  how to beat the state lottery.  So you might want to pay
  attention---this guy knows a thing or two.} bound is a hammer that
you can use to nail a great many problems.  Roughly, the Chernoff
bound says that certain random variables are very unlikely to
significantly exceed their expectation.  For example, if the expected
load on a computer is just a bit below its capacity, then that
computer is unlikely to be overloaded, provided the conditions of the
Chernoff bound are satisfied.

More precisely, the Chernoff Bound says that \emph{the sum of lots of
  little, independent random variables is unlikely to significantly
  exceed the mean of the sum}.  The Markov and Chebyshev bounds lead
to the same kind of conclusion but typically provide much weaker
bounds.  In particular, the Markov and Chebyshev bounds are
polynomial, while the Chernoff bound is exponential.

Here is the theorem.  The proof will come later in
Section~\ref{sec:chernoff_proof}.

\begin{theorem}[\idx{Chernoff Bound}]
\label{chernoff}
Let $T_1, \dots T_n$ be mutually independent random variables such
that $0 \leq T_i \leq 1$ for all $i$.  Let $T = T_1 + \cdots + T_n$.
Then for all $c \geq 1$,
\begin{equation}\label{chernoff-leq}
\prob{T \geq c \expect{T}} \leq e^{-k \expect{T}}
\end{equation}
where $k = c \ln(c) - c + 1$.
\end{theorem}

The Chernoff bound applies only to distributions of sums of
independent random variables that take on values in the interval $[0,
  1]$.  The binomial distribution is of course such a distribution,
but there are lots of other distributions because the Chernoff bound
allows the variables in the sum to have differing, arbitrary, and even
unknown distributions over the range $[0, 1]$.  Furthermore, there is
no direct dependence on the number of random variables in the sum or
their expectations.  In short, the Chernoff bound gives strong results
for lots of problems based on little information---no wonder it is
widely used!

\subsubsection{More Examples}

The Chernoff bound is pretty easy to apply, though the details can be
daunting at first.  Let's walk through a simple example to get the
hang of it.

What is the probability that the number of heads that come up in
1000~independent tosses of a fair coin exceeds the expectation by 20\%
or more?  Let $T_i$ be an indicator variable for the event that the
$i$-th coin is heads.  Then the total number of heads is
\begin{equation*}
    T = T_1 + \cdots + T_{1000}.
\end{equation*}
The Chernoff bound requires that the random variables~$T_i$ be
\idx{mutually independent} and take on values in the range $[0, 1]$.
Both conditions hold here.  In fact, this example is similar to many
applications of the Chernoff bound in that every $T_i$ is
\emph{either} 0 or 1, since they're indicators.

The goal is to bound the probability that the number of heads exceeds
its expectation by 20\% or more; that is, to bound $\prob{T \geq c
  \expect{T}}$ where c = $1.2$.  To that end, we compute $k$ as
defined in the theorem:
\[
k = c \ln(c) - c + 1 = 0.0187\dots.
\]
Plugging this value into the Chernoff bound gives:
\begin{align*}
\Prob{T \geq 1.2 \expect{T}} & \leq  e^{- k \expect{T}} \\
  & = e^{- (0.0187\dots) \cdot 500} \\
  & <  0.0000834.
\end{align*}
So the probability of getting 20\% or more extra heads on 1000 coins
is less than 1 in 10,000.\footnote{Since we are analyzing a binomial
  distribution here, we can get somewhat better bounds using the
  methods from Section~\ref{binomial_distribution_section}, but it is
  much easier to use the Chernoff bounds, and they provide results
  that are nearly as good.}

The bound becomes much stronger as the number of coins increases,
because the expected number of heads appears in the exponent of the
upper bound.  For example, the probability of getting at least 20\%
extra heads on a million coins is at most
\[
e^{- (0.0187\dots) \cdot 500000} < e^{-9392}
\]
which is pretty darn small.

Alternatively, the bound also becomes stronger for larger deviations.
For example, suppose we're interested in the odds of getting 30\% or
more extra heads in 1000 tosses, rather than 20\%.  In that case,
$c= 1.3$ instead of $1.2$.  Consequently, the parameter $k$ rises from
$0.0187$ to about $0.0410$, which may seem insignificant.  But because
$k$ appears in the exponent of the upper bound, the final probability
decreases from around 1 in 10,000 to about 1 in a billion!

\subsubsection{Pick-4}

\idx{Pick-4} is a lottery game where you pick a 4-digit number between
0000 and 9999.  If your number comes up in a random drawing, then you
win~\$5,000.  Your chance of winning is 1 in 10,000.  And if 10
million people play, then the expected number of winners is 1000.  The
lottery operator's nightmare is that the number of winners is much
greater; say, 2000 or more.  What is the probability that will happen?

Let $T_i$ be an indicator for the event that the $i$-th player wins.
Then $T = T_1 + \cdots + T_n$ is the total number of winners.  If we
assume\footnote{As we noted in Chapter~\ref{chap:expectation}, human
  choices are often not uniform and they can be highly dependent.  For
  example, lots of people will pick an important date.  So the lottery
  folks should not get too much comfort from the analysis that
  follows, unless they assign random 4-digit numbers to each player.}
that the players' picks and the winning number are random, independent
and uniform, then the indicators $T_i$~are independent, as required by
the Chernoff bound.


Since 2000~winners would be twice the expected number, we choose $c =
2$, compute $k = c \ln(c) - c + 1 = 0.386\dots$, and plug these values
into the Chernoff bound:
\begin{align*}
\prob{T \geq 2000} & = \Prob{T \geq 2 \expect{T}} \\
  & \leq e^{-k \expect{T}} \\
  & = e^{- (0.386\dots) \cdot 1000} \\
  & < e^{-386}.
\end{align*}
So there is almost no chance that the lottery operator pays out
double.  In fact, the number of winners won't even be 10\% higher than
expected very often.  To prove that, let $c = 1.1$, compute $k = c
\ln(c) - c + 1 = 0.00484\dots$, and plug in again:
\begin{align*}
\Prob{T \geq 1.1 \expect{T}} & \leq e^{-k \expect{T}} \\
  & = e^{- 0.00484\dots * 1000} \\
  & < 0.01.
\end{align*}
So the Pick-4 lottery may be exciting for the players, but the lottery
operator has little doubt about the outcome!

\subsubsection{Randomized Load Balancing}

Now let's return to Fussbook and its \idx{load balancing} problem.
Specifically, we need to determine how many machines suffice to ensure
that no server is overloaded; that is, assigned to do more than 10
minutes of work in a 10-minute interval.

To begin, let's find the probability that the first server is
overloaded.  Let $T_i$ be the number of seconds that the first server
spends on the $i$-th task.  So $T_i$ is zero if the task is assigned
to another machine, and otherwise $T_i$ is the length of the task.
Then $T = \sum_{i = 1}^n T_i$ is the total length of tasks assigned to
the server, where~$n = 24{,}000$.  We need an upper bound on~$\prob{T
  \geq 600}$; that is, the probability that the first server is
assigned more than 600 seconds (or, equivalently, 10 minutes) of work.

The Chernoff bound is applicable only if the $T_i$ are mutually
independent and take on values in the range $[0, 1]$.  The first
condition is satisfied if we assume that task lengths and assignments
are independent.  And the second condition is satisfied because
processing even the most interminable harangue takes at most 1 second.

In all, there are 24,000 tasks, each with an expected length of~1/4
second.  Since tasks are assigned to computers at random, the expected
load on the first server is:
\begin{align}
\expect{T} & = \frac{24{,}000 \mbox{ tasks} \cdot 1/4 \mbox{ second per task}}
  {m \mbox{ machines}} \notag\\
  & = 6000 / m\,\text{seconds}.
\label{eqn:19XX}
\end{align}
For example, if there are $m = 10$ machines, then the expected load on
the first server is 600 seconds, which is 100\% of its capacity.

Now we can use the Chernoff bound to upper bound the probability that
the first server is overloaded:
\begin{align*}
\Prob{T \geq 600}
    &= \Prob{ T \ge \frac{m}{10} \expect{T} } \\
    &= \Prob{T \geq c \expect{T}} \\
    &\leq e^{-(c \ln(c) - c + 1) \cdot 6000 / m},
\end{align*}
where~$c = m/10$. The first equality follows from
Equation~\ref{eqn:19XX}.

\dmj{We didn't state the union bound as a theorem.  We just stated it
  and talked about how it could be proved.}
The probability that \emph{some} server is overloaded is at most
$m$~times the probability that the first server is overloaded by the
union bound in Section~\ref{sec:union_bound}.  So
\begin{align*}
\prob{\text{some server is overloaded}}
    &\le \sum_{i = 1}^m \prob{\text{server $i$ is overloaded}} \\
    &= m \prob{\text{the first server is overloaded}} \\
    &\leq m e^{-(c \ln(c) - c + 1) \cdot 6000 / m},
\end{align*}
where $c = m/10$. Some values of this upper bound are tabulated below:
\[
\begin{array}{rcll}
m & = & 11: & 0.784\dots \\
m & = & 12: & 0.000999\dots \\
m & = & 13: & 0.0000000760\dots
\end{array}
\]
These values suggest that a system with $m = 11$ machines might suffer
immediate overload, $m = 12$ machines could fail in a few days, but $m
= 13$ should be fine for a century or two!

\subsection{Proof of the Chernoff Bound}\label{sec:chernoff_proof}

The proof of the Chernoff bound is somewhat involved.  Heck, even
\emph{Chernoff} didn't come up with it!  His friend, \idx{Herman
  Rubin}, showed him the argument.  Thinking the bound not very
significant, Chernoff did not credit Rubin in print.  He felt pretty
bad when it became famous!\footnote{See ``A Conversation with Herman
  Chernoff,'' \emph{Statistical Science} 1996, Vol.~11, No.~4, pp
  335--350.}

Here is the theorem again, for reference:

\begin{theorem}[\idx{Chernoff Bound}]
Let $T_1, \dots, T_n$ be mutually independent random variables such
that $0 \leq T_i \leq 1$ for all $i$.  Let $T = T_1 + \cdots + T_n$.
Then for all $c \geq 1$,
\begin{equation}\label{chernoff-leq}
\prob{T \geq c \expect{T}} \leq e^{-k \expect{T}}
\end{equation}
where $k = c \ln(c) - c + 1$.
\end{theorem}

\begin{proof} 

For clarity, we'll go through the proof ``top down''; that is, we'll
use facts that are proved immediately afterward.

The key step is to exponentiate both sides of the inequality $T \ge c
\expect{T}$ and then apply the Markov bound:
\begingroup
\openup\jot
\begin{align*}
\prob{T \geq c \expect{T}} & = \prob{c^T \geq c^{c \expect{T}}} \\
  & \leq \frac{\expect{c^T}}{c^{c \expect{T}}} & \text{(by Markov)}\\
  & \leq \frac{e^{(c-1) \expect{T}}}{c^{c \expect{T}}} \\
  & = e^{- (c \ln(c) - c + 1) \expect{T}}.
\end{align*}
\endgroup
In the third step, the numerator is rewritten using the inequality
\[
\expect{c^T} \leq e^{(c-1) \expect{T}}
\]
which is proved below in Lemma~\ref{chernoff-lemma1}.  The final step
is simplification, using the fact that $c^c$~is equal to~$e^{c
  \ln(c)}$.
\end{proof}

Algebra aside, there is a brilliant idea in this proof: in this
context, exponentiating somehow supercharges the \idx{Markov bound}.
This is not true in general!  One unfortunate side-effect is that we
have to bound some nasty expectations involving exponentials in order
to complete the proof.  This is done in the two lemmas below, where
variables take on values as in Theorem~\ref{chernoff}.

\begin{lemma}
\label{chernoff-lemma1}
\[
    \expect{c^T} \leq e^{(c-1) \expect{T}}.
\]
\end{lemma}

\begin{proof}
\begin{align*}
    \expect{c^T} & = \expect{c^{T_1 + \cdots + T_n}} \\
            & = \expect{c^{T_1} \cdots c^{T_n}} \\
            & = \expect{c^{T_1}}  \cdots \expect{c^{T_n}} \\
            & \leq e^{(c-1) \expect{T_1}} \cdots  e^{(c-1) \expect{T_n}} \\
            & = e^{(c-1) (\expect{T_1} + \cdots + \expect{T_n})} \\
            & = e^{(c-1) \expect{T_1 + \cdots + T_n}} \\
            & = e^{(c-1) \expect{T}}.
\end{align*}
The first step uses the definition of $T$, and the second is just
algebra.  The third step uses the fact that the expectation of a
product of independent random variables is the product of the
expectations.  This is where the requirement that the $T_i$~be
independent is used.  Then we bound each term using the inequality
\[
    \expect{c^{T_i}} \leq e^{(c - 1) \expect{T_i}},
\]
which is proved in Lemma~\ref{chernoff-lemma2}.  The last steps are
simplifications using algebra and linearity of expectation.
\end{proof}

\begin{lemma}
\label{chernoff-lemma2}
\[
    \expect{c^{T_i}} \leq e^{(c - 1) \expect{T_i}}
\]
\end{lemma}

\begin{proof}
All summations below range over values $v$ taken by the random
variable~$T_i$, which are all required to be in the interval~$[0, 1]$.
\begin{align*}
\expect{c^{T_i}} & = \sum_v c^v \prob{T_i = v} \\
           & \leq \sum_v (1 + (c-1) v) \prob{T_i = v} \\
           & = \sum_v \prob{T_i = v} + (c-1) v \prob{T_i = v} \\
           & = \sum_v \prob{T_i = v} + \sum(c-1) v \prob{T_i = v} \\
           & = 1 + (c - 1) \sum_v v \prob{T_i = v} \\
           & = 1 + (c - 1) \expect{T_i} \\
           & \leq e^{(c - 1) \expect{T_i}}.
\end{align*}
The first step uses the definition of expectation.  The second step
relies on the inequality $c^v \leq 1 + (c-1) v$, which holds for all
$v$ in~$[0,1]$ and~$c \geq 1$.  This follows from the general
principle that a \idx{convex function}, namely $c^v$, is less than the
linear function, $1 + (c-1) v$, between their points of intersection,
namely $v = 0$ and $1$.  This inequality is why the variables $T_i$
are restricted to the interval $[0, 1]$.  We then multiply out inside
the summation and split into two sums.  The first sum adds the
probabilities of all possible outcomes, so it is equal to 1.  After
pulling the constant $c - 1$ out of the second sum, we're left with
the definition of $\expect{T_i}$.  The final step uses the standard
inequality $1 + z \leq e^z$, which holds for all~$z > 0$.
\end{proof}

\section{Mutually Independent Events}

Suppose that we have a collection of mutually independent events $A_1$,
$A_2$, \dots, $A_n$, and we want to know how many of the events are
likely to occur.

Let $T_i$ be the indicator random variable for~$A_i$ and define
\begin{equation*}
    p_i = \prob{T_i = 1} = \Prob{A_i}
\end{equation*}
for $1 \le i \le n$.  Define
\begin{equation*}
    T = T_1 + T_2 + \dots + T_n
\end{equation*}
to be the number of events that occur.

We know from Linearity of Expectation that
\begin{align*}
\expect{T}
    &= \expect{T_1} + \expect{T_2} + \dots + \expect{T_n} \\
    &= \sum_{i = 1}^n p_i.
\end{align*}
This is true even if the events are \emph{not} independent.

By Theorem~\ref{thm:variance_additivity}, we also know that
\begin{align*}
\variance{T}
     &= \variance{T_1} + \variance{T_2} + \dots + \variance{T_n} \\
     &= \sum_{i = 1}^n p_i (1 - p_i),
\end{align*}
and thus that
\begin{equation*}
    \sigma_T = \sqrt{ \sum_{i = 1}^n p_i (1 - p_i) }.
\end{equation*}
This is true even if the events are only pairwise independent.

Markov's Theorem tells us that for any~$c > 1$,
\begin{equation*}
    \prob{ T \ge c \expect{T} } \le \frac{1}{c}.
\end{equation*}
Chebyshev's Theorem gives us the stronger result that
\begin{equation*}
    \prob{ \abs{T - \expect{T}} \ge c \sigma_T } \le \frac{1}{c^2}.
\end{equation*}

The Chernoff Bound gives us an even stronger result; namely, that for
any~$c > 0$,
\begin{equation*}
\prob{T - \expect{T} \ge c \expect{T}}
    \le e^{-(c \ln(c) - c + 1) \expect{T}}.
\end{equation*}
In this case, the probability of exceeding the mean by~$c \expect{T}$
decreases as an exponentially small function of the deviation.

By considering the random variable~$n - T$, we can also use the
Chernoff Bound to prove that the probability that $T$~is much lower
than~$\expect{T}$ is also exponentially small.  

\subsection{Murphy's Law}

Suppose we want to know the probability that at least 1~event occurs.
If $\expect{T} < 1$, then Markov's Theorem tells us that
\begin{equation*}
    \prob{T \ge 1} \le \expect{T}.
\end{equation*}

On the other hand, if $\expect{T} \ge 1$, then we can obtain a lower
bound on~$\prob{T \ge 1}$ using a result that we call Murphy's
Law\footnote{This is in reference and deference to the famous saying
  that ``If something can go wrong, it will go wrong.''}.

\begin{theorem}[Murphy's Law]\label{thm:18ML}
Let $A_1$, $A_2$, \dots, $A_n$ be mutually independent events.  Let
$T_i$ be the indicator random variable for~$A_i$ and define
\begin{equation*}
    T \eqdef T_1 + T+2 + \dots + T_n
\end{equation*}
to be the number of events that occur.  Then
\begin{equation*}
    \prob{T = 0} \le e^{- \expect{T}}.
\end{equation*}
\end{theorem}

\begin{proof}
\begin{align*}
\prob{T = 0}
    &= \prob{ \bar{A}_1 \land \bar{A}_2 \land \dots \land \bar{A}_n } \\
    &= \prod_{i = 1}^n \prob{\bar{A}_i}
        && \text{(by independence of $A_i$)} \\
    &= \prod_{i = 1}^n (1 - \prob{A_i}) \\
    &\le \prod_{i = 1}^n e^{-\prob{A_i}}
        && \text{(since $\forall x. 1 - x \le e^{-x}$)} \\
    &= e^{-\sum_{i = 1}^n \prob{A_i}} \\
    &= e^{-\sum_{i = 1}^n \expect{T_i}} 
        && \text{(since $T_i$ is an indicator for $A_i$)} \\
    &= e^{-\expect{T}}
        && \text{(Linearity of Expectation)}
        & \qedhere
\end{align*}
\end{proof}

For example, given any set of mutually independent events, if you
expect 10 of them to happen, then at least one of them will happen
with probability at least~$1 - e^{-10}$.  The probability that none of
them happen is at most~$e^{-10} < 1/22000$.

So if there are a lot of independent things that can go wrong and their
probabilities sum to a number much greater than~1, then
Theorem~\ref{thm:18ML} proves that some of them surely will go wrong.

This result can help to explain ``coincidences'' or ``miracles'' or
crazy events that seem to have been very unlikely to happen.  Such
events do happen, in part, because there are so many possible unlikely
events that the sum of their probabilities is greater than~one.  For
example, someone \emph{does} win the lottery.

In fact, if there are 100,000 random tickets in Pick-4,
Theorem~\ref{thm:18ML} says that the probability that there is no
winner is less than~$e^{-10} < 1/22000$.  More generally, there are
literally millions of one-in-a-million possible events and so some of
them will surely occur.

\subsection{Another Magic Trick}

Theorem~\ref{thm:18ML} is surprisingly powerful.  In fact, it is so
powerful that it can enable us to read your mind.  Here's how.

You choose a secret number~$n$ from 1 to~9.  Then we randomly shuffle
an ordinary deck of 52~cards and display the cards one at a time.  You
watch as we reveal the cards and when we reveal the $n$th~card, that
card becomes your secret \emph{card}.  If the card is an Ace, a 10, or
a face card, then you assign that card a \emph{value} of~1.
Otherwise, you assign that card a value that is its number.  For
example, the $J\hea$~gets assigned a value~$v_1 = 1$ and the $4
\dia$~gets assigned a value~$v_1 = 4$. You do all of this in your mind
so that we can't tell when the $n$th~card shows up.

We keep revealing the cards, and when the ($n + v_1$)th card shows up,
that card becomes your \emph{new} secret card.  You compute its
value~$v_2$ using the same scheme as for~$v_1$.  For example, if your
new secret card is the~$10 \clu$, then~$v_2 = 1$.

We proceed in this fashion until all 52~cards have been revealed,
whereupon we read your mind by predicting your last secret card!  How
is this possible?

For the purposes of illustration, suppose that your secret number
was~$n = 3$ and the deck consisted of the 11~cards:
\begin{equation*}
    3\dia \quad 5\spa \quad 2\dia \quad 3\clu \quad 10\clu
    \quad Q\dia \quad 3\hea \quad 7\spa \quad 6\clu
    \quad 4\dia \quad 2\hea.
\end{equation*}
Then your secret cards would be
\begin{equation*}
    2\dia, \, 10\clu, \, Q\dia, \, 3\hea, \, 4\dia
\end{equation*}
since $v_1 = 2$, \ $v_2 = 1$, \ $v_3 = 1$, \ $v_4 = 3$, and~$v_5 =
4$.  In this example, your last secret card is the~$4\dia$.

To make the trick, we follow the same rules as you, except that we
start with~$n = 1$  With the 11-card deck shown above, our secret
cards would be
\begin{equation*}
    3\dia, \, 2\clu, \, 3\hea, \, 4\dia.
\end{equation*}
We have the same last secret card as you do!  That is \emph{not} a
coincidence.  In fact, this is how we predict your last card---we just
guess that it is the same as our last card.  And, we will be right
with probability greater than~90\%.

To see why the trick is likely to work, you need to notice that if we
ever share a secret card, then we will surely have the same
\emph{last} secret card.  That's because we will perform exactly the
same steps as the cards are revealed.

Each time we get a new secret card, there is always a chance that it
was one of your secret cards.  For any given step, the chance of a
match is small but we get a lot of chances.  In fact, the number of
chances will typically outweigh the inverse of the probability of a
match on any given step and so, at least informally, Murphy's Law
suggests that we are likely to eventually get a match, whereupon we
can read your mind.

The details of the proof are complicated and we will not present them
here.  One of the main complications is that when you are revealing
cards from a deck without replacement, the probability of getting a
match on a given step is conditional based on the cards that have
already been revealed.

\problemsection

\endinput

